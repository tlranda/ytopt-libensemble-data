“RANK= 12 LOCAL_RANK= 0 gpu= 3”
“RANK= 13 LOCAL_RANK= 1 gpu= 2”
“RANK= 14 LOCAL_RANK= 2 gpu= 1”
“RANK= 15 LOCAL_RANK= 3 gpu= 0”
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ speed3d_r2c cufft float 128 128 128 -reorder -p2p_pl -r2c_dir 0 -ingrid 8 2 1 -outgrid 4 4 1
Shell debugging restarted
Shell debugging restarted
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
+ unset __lmod_vx
+ speed3d_r2c cufft float 128 128 128 -reorder -p2p_pl -r2c_dir 0 -ingrid 8 2 1 -outgrid 4 4 1
+ speed3d_r2c cufft float 128 128 128 -reorder -p2p_pl -r2c_dir 0 -ingrid 8 2 1 -outgrid 4 4 1
+ speed3d_r2c cufft float 128 128 128 -reorder -p2p_pl -r2c_dir 0 -ingrid 8 2 1 -outgrid 4 4 1
“RANK= 6 LOCAL_RANK= 2 gpu= 1”
“RANK= 7 LOCAL_RANK= 3 gpu= 0”
“RANK= 5 LOCAL_RANK= 1 gpu= 2”
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 128 128 128 -reorder -p2p_pl -r2c_dir 0 -ingrid 8 2 1 -outgrid 4 4 1
+ speed3d_r2c cufft float 128 128 128 -reorder -p2p_pl -r2c_dir 0 -ingrid 8 2 1 -outgrid 4 4 1
+ speed3d_r2c cufft float 128 128 128 -reorder -p2p_pl -r2c_dir 0 -ingrid 8 2 1 -outgrid 4 4 1
“RANK= 4 LOCAL_RANK= 0 gpu= 3”
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 128 128 128 -reorder -p2p_pl -r2c_dir 0 -ingrid 8 2 1 -outgrid 4 4 1
“RANK= 8 LOCAL_RANK= 0 gpu= 3”
“RANK= 10 LOCAL_RANK= 2 gpu= 1”
“RANK= 9 LOCAL_RANK= 1 gpu= 2”
“RANK= 11 LOCAL_RANK= 3 gpu= 0”
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 128 128 128 -reorder -p2p_pl -r2c_dir 0 -ingrid 8 2 1 -outgrid 4 4 1
+ speed3d_r2c cufft float 128 128 128 -reorder -p2p_pl -r2c_dir 0 -ingrid 8 2 1 -outgrid 4 4 1
+ speed3d_r2c cufft float 128 128 128 -reorder -p2p_pl -r2c_dir 0 -ingrid 8 2 1 -outgrid 4 4 1
+ speed3d_r2c cufft float 128 128 128 -reorder -p2p_pl -r2c_dir 0 -ingrid 8 2 1 -outgrid 4 4 1
“RANK= 3 LOCAL_RANK= 3 gpu= 0”
“RANK= 2 LOCAL_RANK= 2 gpu= 1”
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 128 128 128 -reorder -p2p_pl -r2c_dir 0 -ingrid 8 2 1 -outgrid 4 4 1
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 128 128 128 -reorder -p2p_pl -r2c_dir 0 -ingrid 8 2 1 -outgrid 4 4 1
“RANK= 1 LOCAL_RANK= 1 gpu= 2”
“RANK= 0 LOCAL_RANK= 0 gpu= 3”
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 128 128 128 -reorder -p2p_pl -r2c_dir 0 -ingrid 8 2 1 -outgrid 4 4 1
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 128 128 128 -reorder -p2p_pl -r2c_dir 0 -ingrid 8 2 1 -outgrid 4 4 1
MPICH ERROR [Rank 10] [job id d22d66c5-32c9-4e4b-bce4-b1f42cd3fbc6] [Thu Jun 29 02:44:25 2023] [x3111c0s13b0n0] - Abort(606220687) (rank 10 in comm 0): Fatal error in PMPI_Waitany: Other MPI error, error stack:
PMPI_Waitany(277)..............: MPI_Waitany(count=3, req_array=0x1ee0c60, index=0x7ffe126b0344, status=0x1) failed
PMPI_Waitany(245)..............: 
MPIR_Waitany_impl(97)..........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Waitany: Other MPI error, error stack:
PMPI_Waitany(277)..............: MPI_Waitany(count=3, req_array=0x1ee0c60, index=0x7ffe126b0344, status=0x1) failed
PMPI_Waitany(245)..............: 
MPIR_Waitany_impl(97)..........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 15] [job id d22d66c5-32c9-4e4b-bce4-b1f42cd3fbc6] [Thu Jun 29 02:44:25 2023] [x3111c0s13b1n0] - Abort(404894095) (rank 15 in comm 0): Fatal error in PMPI_Waitany: Other MPI error, error stack:
PMPI_Waitany(277)..............: MPI_Waitany(count=3, req_array=0x1a20390, index=0x7ffdc4aae154, status=0x1) failed
PMPI_Waitany(245)..............: 
MPIR_Waitany_impl(97)..........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Waitany: Other MPI error, error stack:
PMPI_Waitany(277)..............: MPI_Waitany(count=3, req_array=0x1a20390, index=0x7ffdc4aae154, status=0x1) failed
PMPI_Waitany(245)..............: 
MPIR_Waitany_impl(97)..........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
x3111c0s13b0n0.hsn.cm.polaris.alcf.anl.gov: rank 10 exited with code 255
(GTL DEBUG: 8) cuIpcOpenMemHandle: invalid resource handle, CUDA_ERROR_INVALID_HANDLE, line no 272
MPICH ERROR [Rank 8] [job id d22d66c5-32c9-4e4b-bce4-b1f42cd3fbc6] [Thu Jun 29 02:44:25 2023] [x3111c0s13b0n0] - Abort(473540098) (rank 8 in comm 0): Fatal error in PMPI_Irecv: Invalid count, error stack:
PMPI_Irecv(166)......................: MPI_Irecv(buf=0x14712ba88000, count=16384, dtype=0x4c000840, src=10, tag=0, MPI_COMM_WORLD, request=0x1872c30) failed
MPID_Irecv(497)......................: 
MPIDI_irecv_unsafe(160)..............: 
MPIDI_SHM_mpi_irecv(462).............: 
MPIDI_SHM_mpi_imrecv(514)............: 
MPIDI_SHM_mmods_try_matched_recv(167): 
MPIDI_CRAY_Common_lmt_handle_recv(44): 
MPIDI_CRAY_Common_lmt_import_mem(218): 
(unknown)(): Invalid count

aborting job:
Fatal error in PMPI_Irecv: Invalid count, error stack:
PMPI_Irecv(166)......................: MPI_Irecv(buf=0x14712ba88000, count=16384, dtype=0x4c000840, src=10, tag=0, MPI_COMM_WORLD, request=0x1872c30) failed
MPID_Irecv(497)......................: 
MPIDI_irecv_unsafe(160)..............: 
MPIDI_SHM_mpi_irecv(462).............: 
MPIDI_SHM_mpi_imrecv(514)............: 
MPIDI_SHM_mmods_try_matched_recv(167): 
MPIDI_CRAY_Common_lmt_handle_recv(44): 
MPIDI_CRAY_Common_lmt_import_mem(218): 
(unknown)(): Invalid count
MPICH ERROR [Rank 6] [job id d22d66c5-32c9-4e4b-bce4-b1f42cd3fbc6] [Thu Jun 29 02:44:25 2023] [x3110c0s7b1n0] - Abort(941207311) (rank 6 in comm 0): Fatal error in PMPI_Isend: Other MPI error, error stack:
PMPI_Isend(161)...........: MPI_Isend(buf=0x1501d1a20000, count=16384, dtype=0x4c000840, dest=10, tag=0, MPI_COMM_WORLD, request=0x972464) failed
MPID_Isend(584)...........: 
MPIDI_isend_unsafe(136)...: 
MPIDI_OFI_send_normal(352): OFI tagged senddata failed (ofi_send.h:352:MPIDI_OFI_send_normal:No route to host)

aborting job:
Fatal error in PMPI_Isend: Other MPI error, error stack:
PMPI_Isend(161)...........: MPI_Isend(buf=0x1501d1a20000, count=16384, dtype=0x4c000840, dest=10, tag=0, MPI_COMM_WORLD, request=0x972464) failed
MPID_Isend(584)...........: 
MPIDI_isend_unsafe(136)...: 
MPIDI_OFI_send_normal(352): OFI tagged senddata failed (ofi_send.h:352:MPIDI_OFI_send_normal:No route to host)
MPICH ERROR [Rank 11] [job id d22d66c5-32c9-4e4b-bce4-b1f42cd3fbc6] [Thu Jun 29 02:44:25 2023] [x3111c0s13b0n0] - Abort(807022991) (rank 11 in comm 0): Fatal error in PMPI_Waitany: Other MPI error, error stack:
PMPI_Waitany(277)..............: MPI_Waitany(count=3, req_array=0xa559e0, index=0x7ffdc773b5f4, status=0x1) failed
PMPI_Waitany(245)..............: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Waitany: Other MPI error, error stack:
PMPI_Waitany(277)..............: MPI_Waitany(count=3, req_array=0xa559e0, index=0x7ffdc773b5f4, status=0x1) failed
PMPI_Waitany(245)..............: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 7] [job id d22d66c5-32c9-4e4b-bce4-b1f42cd3fbc6] [Thu Jun 29 02:44:25 2023] [x3110c0s7b1n0] - Abort(739880719) (rank 7 in comm 0): Fatal error in PMPI_Isend: Other MPI error, error stack:
PMPI_Isend(161)...........: MPI_Isend(buf=0x153763a60000, count=16384, dtype=0x4c000840, dest=15, tag=0, MPI_COMM_WORLD, request=0x1f2fe0c) failed
MPID_Isend(584)...........: 
MPIDI_isend_unsafe(136)...: 
MPIDI_OFI_send_normal(352): OFI tagged senddata failed (ofi_send.h:352:MPIDI_OFI_send_normal:No route to host)

aborting job:
Fatal error in PMPI_Isend: Other MPI error, error stack:
PMPI_Isend(161)...........: MPI_Isend(buf=0x153763a60000, count=16384, dtype=0x4c000840, dest=15, tag=0, MPI_COMM_WORLD, request=0x1f2fe0c) failed
MPID_Isend(584)...........: 
MPIDI_isend_unsafe(136)...: 
MPIDI_OFI_send_normal(352): OFI tagged senddata failed (ofi_send.h:352:MPIDI_OFI_send_normal:No route to host)
MPICH ERROR [Rank 2] [job id d22d66c5-32c9-4e4b-bce4-b1f42cd3fbc6] [Thu Jun 29 02:44:25 2023] [x3110c0s7b0n0] - Abort(1009398159) (rank 2 in comm 0): Fatal error in PMPI_Waitany: Other MPI error, error stack:
PMPI_Waitany(277)..............: MPI_Waitany(count=4, req_array=0x14874e0, index=0x7ffdac197554, status=0x1) failed
PMPI_Waitany(245)..............: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Waitany: Other MPI error, error stack:
PMPI_Waitany(277)..............: MPI_Waitany(count=4, req_array=0x14874e0, index=0x7ffdac197554, status=0x1) failed
PMPI_Waitany(245)..............: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 3] [job id d22d66c5-32c9-4e4b-bce4-b1f42cd3fbc6] [Thu Jun 29 02:44:25 2023] [x3110c0s7b0n0] - Abort(3289487) (rank 3 in comm 0): Fatal error in PMPI_Waitany: Other MPI error, error stack:
PMPI_Waitany(277)..............: MPI_Waitany(count=4, req_array=0x10338d0, index=0x7ffc2b419a84, status=0x1) failed
PMPI_Waitany(245)..............: 
MPIR_Waitany_impl(97)..........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Waitany: Other MPI error, error stack:
PMPI_Waitany(277)..............: MPI_Waitany(count=4, req_array=0x10338d0, index=0x7ffc2b419a84, status=0x1) failed
PMPI_Waitany(245)..............: 
MPIR_Waitany_impl(97)..........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 14] [job id d22d66c5-32c9-4e4b-bce4-b1f42cd3fbc6] [Thu Jun 29 02:44:25 2023] [x3111c0s13b1n0] - Abort(2240911) (rank 14 in comm 0): Fatal error in PMPI_Waitany: Other MPI error, error stack:
PMPI_Waitany(277)..............: MPI_Waitany(count=3, req_array=0x16aa390, index=0x7fffe1569d94, status=0x1) failed
PMPI_Waitany(245)..............: 
MPIR_Waitany_impl(97)..........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)

aborting job:
Fatal error in PMPI_Waitany: Other MPI error, error stack:
PMPI_Waitany(277)..............: MPI_Waitany(count=3, req_array=0x16aa390, index=0x7fffe1569d94, status=0x1) failed
PMPI_Waitany(245)..............: 
MPIR_Waitany_impl(97)..........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)
MPICH ERROR [Rank 0] [job id d22d66c5-32c9-4e4b-bce4-b1f42cd3fbc6] [Thu Jun 29 02:44:25 2023] [x3110c0s7b0n0] - Abort(203043215) (rank 0 in comm 0): Fatal error in PMPI_Waitany: Other MPI error, error stack:
PMPI_Waitany(277)..............: MPI_Waitany(count=3, req_array=0x1da2ea0, index=0x7ffdb95c0884, status=0x1) failed
PMPI_Waitany(245)..............: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Waitany: Other MPI error, error stack:
PMPI_Waitany(277)..............: MPI_Waitany(count=3, req_array=0x1da2ea0, index=0x7ffdb95c0884, status=0x1) failed
PMPI_Waitany(245)..............: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
(GTL DEBUG: 13) cuIpcOpenMemHandle: invalid resource handle, CUDA_ERROR_INVALID_HANDLE, line no 272
MPICH ERROR [Rank 13] [job id d22d66c5-32c9-4e4b-bce4-b1f42cd3fbc6] [Thu Jun 29 02:44:25 2023] [x3111c0s13b1n0] - Abort(943302146) (rank 13 in comm 0): Fatal error in PMPI_Irecv: Invalid count, error stack:
PMPI_Irecv(166)......................: MPI_Irecv(buf=0x14689ba80000, count=16384, dtype=0x4c000840, src=15, tag=0, MPI_COMM_WORLD, request=0x204c390) failed
MPID_Irecv(497)......................: 
MPIDI_irecv_unsafe(160)..............: 
MPIDI_SHM_mpi_irecv(462).............: 
MPIDI_SHM_mpi_imrecv(514)............: 
MPIDI_SHM_mmods_try_matched_recv(167): 
MPIDI_CRAY_Common_lmt_handle_recv(44): 
MPIDI_CRAY_Common_lmt_import_mem(218): 
(unknown)(): Invalid count

aborting job:
Fatal error in PMPI_Irecv: Invalid count, error stack:
PMPI_Irecv(166)......................: MPI_Irecv(buf=0x14689ba80000, count=16384, dtype=0x4c000840, src=15, tag=0, MPI_COMM_WORLD, request=0x204c390) failed
MPID_Irecv(497)......................: 
MPIDI_irecv_unsafe(160)..............: 
MPIDI_SHM_mpi_irecv(462).............: 
MPIDI_SHM_mpi_imrecv(514)............: 
MPIDI_SHM_mmods_try_matched_recv(167): 
MPIDI_CRAY_Common_lmt_handle_recv(44): 
MPIDI_CRAY_Common_lmt_import_mem(218): 
(unknown)(): Invalid count
MPICH ERROR [Rank 1] [job id d22d66c5-32c9-4e4b-bce4-b1f42cd3fbc6] [Thu Jun 29 02:44:25 2023] [x3110c0s7b0n0] - Abort(471445263) (rank 1 in comm 0): Fatal error in PMPI_Isend: Other MPI error, error stack:
PMPI_Isend(161)...........: MPI_Isend(buf=0x146bf1a40000, count=16384, dtype=0x4c000840, dest=7, tag=0, MPI_COMM_WORLD, request=0x11d91a8) failed
MPID_Isend(584)...........: 
MPIDI_isend_unsafe(136)...: 
MPIDI_OFI_send_normal(352): OFI tagged senddata failed (ofi_send.h:352:MPIDI_OFI_send_normal:No route to host)

aborting job:
Fatal error in PMPI_Isend: Other MPI error, error stack:
PMPI_Isend(161)...........: MPI_Isend(buf=0x146bf1a40000, count=16384, dtype=0x4c000840, dest=7, tag=0, MPI_COMM_WORLD, request=0x11d91a8) failed
MPID_Isend(584)...........: 
MPIDI_isend_unsafe(136)...: 
MPIDI_OFI_send_normal(352): OFI tagged senddata failed (ofi_send.h:352:MPIDI_OFI_send_normal:No route to host)
Signal forwarding failed: Application d22d66c5-32c9-4e4b-bce4-b1f42cd3fbc6 not found
