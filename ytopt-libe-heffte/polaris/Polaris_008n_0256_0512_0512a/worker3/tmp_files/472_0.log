“RANK= 20 LOCAL_RANK= 0 gpu= 3”
“RANK= 21 LOCAL_RANK= 1 gpu= 2”
“RANK= 22 LOCAL_RANK= 2 gpu= 1”
“RANK= 23 LOCAL_RANK= 3 gpu= 0”
“RANK= 28 LOCAL_RANK= 0 gpu= 3”
“RANK= 29 LOCAL_RANK= 1 gpu= 2”
“RANK= 30 LOCAL_RANK= 2 gpu= 1”
“RANK= 31 LOCAL_RANK= 3 gpu= 0”
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ unset __lmod_vx
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
“RANK= 4 LOCAL_RANK= 0 gpu= 3”
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
“RANK= 5 LOCAL_RANK= 1 gpu= 2”
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
“RANK= 7 LOCAL_RANK= 3 gpu= 0”
“RANK= 12 LOCAL_RANK= 0 gpu= 3”
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
“RANK= 6 LOCAL_RANK= 2 gpu= 1”
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
“RANK= 26 LOCAL_RANK= 2 gpu= 1”
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 512 512 -no-reorder -r2c_dir 1 -ingrid 16 1 2 -outgrid 16 2 1 -n5
“RANK= 25 LOCAL_RANK= 1 gpu= 2”
Shell debugging restarted
+ unset __lmod_vx
“RANK= 14 LOCAL_RANK= 2 gpu= 1”
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 512 512 -no-reorder -r2c_dir 1 -ingrid 16 1 2 -outgrid 16 2 1 -n5
“RANK= 24 LOCAL_RANK= 0 gpu= 3”
Shell debugging restarted
+ unset __lmod_vx
“RANK= 8 LOCAL_RANK= 0 gpu= 3”
“RANK= 13 LOCAL_RANK= 1 gpu= 2”
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 512 512 -no-reorder -r2c_dir 1 -ingrid 16 1 2 -outgrid 16 2 1 -n5
“RANK= 27 LOCAL_RANK= 3 gpu= 0”
+ speed3d_r2c cufft float 256 512 512 -no-reorder -r2c_dir 1 -ingrid 16 1 2 -outgrid 16 2 1 -n5
“RANK= 9 LOCAL_RANK= 1 gpu= 2”
“RANK= 15 LOCAL_RANK= 3 gpu= 0”
“RANK= 16 LOCAL_RANK= 0 gpu= 3”
+ speed3d_r2c cufft float 256 512 512 -no-reorder -r2c_dir 1 -ingrid 16 1 2 -outgrid 16 2 1 -n5
+ speed3d_r2c cufft float 256 512 512 -no-reorder -r2c_dir 1 -ingrid 16 1 2 -outgrid 16 2 1 -n5
“RANK= 11 LOCAL_RANK= 3 gpu= 0”
+ speed3d_r2c cufft float 256 512 512 -no-reorder -r2c_dir 1 -ingrid 16 1 2 -outgrid 16 2 1 -n5
“RANK= 10 LOCAL_RANK= 2 gpu= 1”
+ speed3d_r2c cufft float 256 512 512 -no-reorder -r2c_dir 1 -ingrid 16 1 2 -outgrid 16 2 1 -n5
“RANK= 19 LOCAL_RANK= 3 gpu= 0”
“RANK= 17 LOCAL_RANK= 1 gpu= 2”
“RANK= 18 LOCAL_RANK= 2 gpu= 1”
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ '[' -z '' ']'
+ case "$-" in
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ unset __lmod_vx
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 512 512 -no-reorder -r2c_dir 1 -ingrid 16 1 2 -outgrid 16 2 1 -n5
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ speed3d_r2c cufft float 256 512 512 -no-reorder -r2c_dir 1 -ingrid 16 1 2 -outgrid 16 2 1 -n5
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 512 512 -no-reorder -r2c_dir 1 -ingrid 16 1 2 -outgrid 16 2 1 -n5
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ speed3d_r2c cufft float 256 512 512 -no-reorder -r2c_dir 1 -ingrid 16 1 2 -outgrid 16 2 1 -n5
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ '[' -z '' ']'
+ case "$-" in
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ speed3d_r2c cufft float 256 512 512 -no-reorder -r2c_dir 1 -ingrid 16 1 2 -outgrid 16 2 1 -n5
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 512 512 -no-reorder -r2c_dir 1 -ingrid 16 1 2 -outgrid 16 2 1 -n5
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 512 512 -no-reorder -r2c_dir 1 -ingrid 16 1 2 -outgrid 16 2 1 -n5
Shell debugging restarted
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 512 512 -no-reorder -r2c_dir 1 -ingrid 16 1 2 -outgrid 16 2 1 -n5
+ unset __lmod_vx
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ unset __lmod_vx
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 512 512 -no-reorder -r2c_dir 1 -ingrid 16 1 2 -outgrid 16 2 1 -n5
Shell debugging restarted
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 512 512 -no-reorder -r2c_dir 1 -ingrid 16 1 2 -outgrid 16 2 1 -n5
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 512 512 -no-reorder -r2c_dir 1 -ingrid 16 1 2 -outgrid 16 2 1 -n5
+ speed3d_r2c cufft float 256 512 512 -no-reorder -r2c_dir 1 -ingrid 16 1 2 -outgrid 16 2 1 -n5
+ speed3d_r2c cufft float 256 512 512 -no-reorder -r2c_dir 1 -ingrid 16 1 2 -outgrid 16 2 1 -n5
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 512 512 -no-reorder -r2c_dir 1 -ingrid 16 1 2 -outgrid 16 2 1 -n5
+ speed3d_r2c cufft float 256 512 512 -no-reorder -r2c_dir 1 -ingrid 16 1 2 -outgrid 16 2 1 -n5
+ speed3d_r2c cufft float 256 512 512 -no-reorder -r2c_dir 1 -ingrid 16 1 2 -outgrid 16 2 1 -n5
+ speed3d_r2c cufft float 256 512 512 -no-reorder -r2c_dir 1 -ingrid 16 1 2 -outgrid 16 2 1 -n5
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 512 512 -no-reorder -r2c_dir 1 -ingrid 16 1 2 -outgrid 16 2 1 -n5
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 512 512 -no-reorder -r2c_dir 1 -ingrid 16 1 2 -outgrid 16 2 1 -n5
+ speed3d_r2c cufft float 256 512 512 -no-reorder -r2c_dir 1 -ingrid 16 1 2 -outgrid 16 2 1 -n5
“RANK= 0 LOCAL_RANK= 0 gpu= 3”
“RANK= 1 LOCAL_RANK= 1 gpu= 2”
“RANK= 2 LOCAL_RANK= 2 gpu= 1”
“RANK= 3 LOCAL_RANK= 3 gpu= 0”
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 512 512 -no-reorder -r2c_dir 1 -ingrid 16 1 2 -outgrid 16 2 1 -n5
+ speed3d_r2c cufft float 256 512 512 -no-reorder -r2c_dir 1 -ingrid 16 1 2 -outgrid 16 2 1 -n5
+ speed3d_r2c cufft float 256 512 512 -no-reorder -r2c_dir 1 -ingrid 16 1 2 -outgrid 16 2 1 -n5
+ speed3d_r2c cufft float 256 512 512 -no-reorder -r2c_dir 1 -ingrid 16 1 2 -outgrid 16 2 1 -n5
MPICH ERROR [Rank 4] [job id b711ba51-b89f-471f-b8c7-7a2d81744b0c] [Sun Nov 12 08:56:03 2023] [x3001c0s19b1n0] - Abort(204544655) (rank 4 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x1475ac000000, scnts=0x15bcd80, sdispls=0x15bd8c0, dtype=0x4c000840, rbuf=0x1475ac810000, rcnts=0x15bd9e0, rdispls=0x15bda70, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x1475ac000000, scnts=0x15bcd80, sdispls=0x15bd8c0, dtype=0x4c000840, rbuf=0x1475ac810000, rcnts=0x15bd9e0, rdispls=0x15bda70, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 10] [job id b711ba51-b89f-471f-b8c7-7a2d81744b0c] [Sun Nov 12 08:56:03 2023] [x3001c0s1b0n0] - Abort(70326927) (rank 10 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x149ba8000000, scnts=0x1258060, sdispls=0x123c930, dtype=0x4c000840, rbuf=0x149ba8810000, rcnts=0x12589a0, rdispls=0x1258a30, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x149ba8000000, scnts=0x1258060, sdispls=0x123c930, dtype=0x4c000840, rbuf=0x149ba8810000, rcnts=0x12589a0, rdispls=0x1258a30, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 29] [job id b711ba51-b89f-471f-b8c7-7a2d81744b0c] [Sun Nov 12 08:56:03 2023] [x3001c0s31b1n0] - Abort(204544655) (rank 29 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x153fb2000000, scnts=0x15e9060, sdispls=0x15aff70, dtype=0x4c000840, rbuf=0x153fb2800000, rcnts=0x15efe80, rdispls=0x161cef0, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x153fb2000000, scnts=0x15e9060, sdispls=0x15aff70, dtype=0x4c000840, rbuf=0x153fb2800000, rcnts=0x15efe80, rdispls=0x161cef0, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)
MPICH ERROR [Rank 30] [job id b711ba51-b89f-471f-b8c7-7a2d81744b0c] [Sun Nov 12 08:56:03 2023] [x3001c0s31b1n0] - Abort(204544655) (rank 30 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14e178000000, scnts=0x1fa42f0, sdispls=0x1fa4ba0, dtype=0x4c000840, rbuf=0x14e178800000, rcnts=0x1fa4cc0, rdispls=0x1fa4d50, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14e178000000, scnts=0x1fa42f0, sdispls=0x1fa4ba0, dtype=0x4c000840, rbuf=0x14e178800000, rcnts=0x1fa4cc0, rdispls=0x1fa4d50, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 22] [job id b711ba51-b89f-471f-b8c7-7a2d81744b0c] [Sun Nov 12 08:56:03 2023] [x3001c0s25b1n0] - Abort(674306703) (rank 22 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14bbf8000000, scnts=0x1af4f20, sdispls=0x1afad50, dtype=0x4c000840, rbuf=0x14bbf8800000, rcnts=0x1ab7ac0, rdispls=0x1ab7b50, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14bbf8000000, scnts=0x1af4f20, sdispls=0x1afad50, dtype=0x4c000840, rbuf=0x14bbf8800000, rcnts=0x1ab7ac0, rdispls=0x1ab7b50, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 24] [job id b711ba51-b89f-471f-b8c7-7a2d81744b0c] [Sun Nov 12 08:56:03 2023] [x3001c0s31b0n0] - Abort(338762383) (rank 24 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x146ef8000000, scnts=0x1ecfa10, sdispls=0x1ed0550, dtype=0x4c000840, rbuf=0x146ef8800000, rcnts=0x1ed0670, rdispls=0x1ed0700, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x146ef8000000, scnts=0x1ecfa10, sdispls=0x1ed0550, dtype=0x4c000840, rbuf=0x146ef8800000, rcnts=0x1ed0670, rdispls=0x1ed0700, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 28] [job id b711ba51-b89f-471f-b8c7-7a2d81744b0c] [Sun Nov 12 08:56:03 2023] [x3001c0s31b1n0] - Abort(204544655) (rank 28 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x1538a2000000, scnts=0xb2ef70, sdispls=0xb30f60, dtype=0x4c000840, rbuf=0x1538a2800000, rcnts=0xb0e320, rdispls=0xb0e3b0, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x1538a2000000, scnts=0xb2ef70, sdispls=0xb30f60, dtype=0x4c000840, rbuf=0x1538a2800000, rcnts=0xb0e320, rdispls=0xb0e3b0, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)
MPICH ERROR [Rank 3] [job id b711ba51-b89f-471f-b8c7-7a2d81744b0c] [Sun Nov 12 08:56:03 2023] [x3001c0s19b0n0] - Abort(808524431) (rank 3 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14abe8000000, scnts=0x18f7850, sdispls=0x190fb10, dtype=0x4c000840, rbuf=0x14abe8810000, rcnts=0x18f0930, rdispls=0x18f09c0, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14abe8000000, scnts=0x18f7850, sdispls=0x190fb10, dtype=0x4c000840, rbuf=0x14abe8810000, rcnts=0x18f0930, rdispls=0x18f09c0, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 21] [job id b711ba51-b89f-471f-b8c7-7a2d81744b0c] [Sun Nov 12 08:56:03 2023] [x3001c0s25b1n0] - Abort(540088975) (rank 21 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14a508000000, scnts=0xf3a060, sdispls=0xf00f50, dtype=0x4c000840, rbuf=0x14a508800000, rcnts=0xf3a750, rdispls=0xf3a7e0, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14a508000000, scnts=0xf3a060, sdispls=0xf00f50, dtype=0x4c000840, rbuf=0x14a508800000, rcnts=0xf3a750, rdispls=0xf3a7e0, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)
MPICH ERROR [Rank 9] [job id b711ba51-b89f-471f-b8c7-7a2d81744b0c] [Sun Nov 12 08:56:03 2023] [x3001c0s1b0n0] - Abort(70326927) (rank 9 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x152832000000, scnts=0x9f2930, sdispls=0x8cb930, dtype=0x4c000840, rbuf=0x152832810000, rcnts=0x8aa4f0, rdispls=0x8aa580, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x152832000000, scnts=0x9f2930, sdispls=0x8cb930, dtype=0x4c000840, rbuf=0x152832810000, rcnts=0x8aa4f0, rdispls=0x8aa580, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 12] [job id b711ba51-b89f-471f-b8c7-7a2d81744b0c] [Sun Nov 12 08:56:03 2023] [x3001c0s1b1n0] - Abort(875633295) (rank 12 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x15529a000000, scnts=0x1d6c690, sdispls=0x1d6d1d0, dtype=0x4c000840, rbuf=0x15529a810000, rcnts=0x1d6d2f0, rdispls=0x1d6d380, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x15529a000000, scnts=0x1d6c690, sdispls=0x1d6d1d0, dtype=0x4c000840, rbuf=0x15529a810000, rcnts=0x1d6d2f0, rdispls=0x1d6d380, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 6] [job id b711ba51-b89f-471f-b8c7-7a2d81744b0c] [Sun Nov 12 08:56:03 2023] [x3001c0s19b1n0] - Abort(674306703) (rank 6 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x148b0c000000, scnts=0x1b5b060, sdispls=0x1b5b910, dtype=0x4c000840, rbuf=0x148b0c810000, rcnts=0x1b5ba30, rdispls=0x1b5bac0, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x148b0c000000, scnts=0x1b5b060, sdispls=0x1b5b910, dtype=0x4c000840, rbuf=0x148b0c810000, rcnts=0x1b5ba30, rdispls=0x1b5bac0, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 11] [job id b711ba51-b89f-471f-b8c7-7a2d81744b0c] [Sun Nov 12 08:56:03 2023] [x3001c0s1b0n0] - Abort(271653519) (rank 11 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x15166c000000, scnts=0x25332f0, sdispls=0x2517710, dtype=0x4c000840, rbuf=0x15166c810000, rcnts=0x2533c30, rdispls=0x2533cc0, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x15166c000000, scnts=0x25332f0, sdispls=0x2517710, dtype=0x4c000840, rbuf=0x15166c810000, rcnts=0x2533c30, rdispls=0x2533cc0, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 14] [job id b711ba51-b89f-471f-b8c7-7a2d81744b0c] [Sun Nov 12 08:56:03 2023] [x3001c0s1b1n0] - Abort(3218063) (rank 14 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14e26c000000, scnts=0xf30f80, sdispls=0xf30930, dtype=0x4c000840, rbuf=0x14e26c810000, rcnts=0xf2ef20, rdispls=0xef1210, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14e26c000000, scnts=0xf30f80, sdispls=0xf30930, dtype=0x4c000840, rbuf=0x14e26c810000, rcnts=0xf2ef20, rdispls=0xef1210, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 1] [job id b711ba51-b89f-471f-b8c7-7a2d81744b0c] [Sun Nov 12 08:56:03 2023] [x3001c0s19b0n0] - Abort(674306703) (rank 1 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14ef18000000, scnts=0x24f9700, sdispls=0x24bfcf0, dtype=0x4c000840, rbuf=0x14ef18810000, rcnts=0x24bc2a0, rdispls=0x24bc330, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14ef18000000, scnts=0x24f9700, sdispls=0x24bfcf0, dtype=0x4c000840, rbuf=0x14ef18810000, rcnts=0x24bc2a0, rdispls=0x24bc330, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 27] [job id b711ba51-b89f-471f-b8c7-7a2d81744b0c] [Sun Nov 12 08:56:03 2023] [x3001c0s31b0n0] - Abort(808524431) (rank 27 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x154878000000, scnts=0x7c3670, sdispls=0x7c5930, dtype=0x4c000840, rbuf=0x154878800000, rcnts=0x786210, rdispls=0x7862a0, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x154878000000, scnts=0x7c3670, sdispls=0x7c5930, dtype=0x4c000840, rbuf=0x154878800000, rcnts=0x786210, rdispls=0x7862a0, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)
MPICH ERROR [Rank 2] [job id b711ba51-b89f-471f-b8c7-7a2d81744b0c] [Sun Nov 12 08:56:03 2023] [x3001c0s19b0n0] - Abort(271653519) (rank 2 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x1491ec000000, scnts=0x80e7c0, sdispls=0x826b10, dtype=0x4c000840, rbuf=0x1491ec810000, rcnts=0x807930, rdispls=0x8079c0, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x1491ec000000, scnts=0x80e7c0, sdispls=0x826b10, dtype=0x4c000840, rbuf=0x1491ec810000, rcnts=0x807930, rdispls=0x8079c0, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 19] [job id b711ba51-b89f-471f-b8c7-7a2d81744b0c] [Sun Nov 12 08:56:03 2023] [x3001c0s25b0n0] - Abort(204020239) (rank 19 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389).........: MPI_Alltoallv(sbuf=0x15115c000000, scnts=0x16ecaa0, sdispls=0x16eaf80, dtype=0x4c000840, rbuf=0x15115c800000, rcnts=0x16ad410, rdispls=0x16ad4a0, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1155)...: 
MPIC_Isend(511).............: 
MPID_Isend_coll(610)........: 
MPIDI_isend_coll_unsafe(176): 
MPIDI_OFI_send_normal(352)..: OFI tagged senddata failed (ofi_send.h:352:MPIDI_OFI_send_normal:No route to host)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389).........: MPI_Alltoallv(sbuf=0x15115c000000, scnts=0x16ecaa0, sdispls=0x16eaf80, dtype=0x4c000840, rbuf=0x15115c800000, rcnts=0x16ad410, rdispls=0x16ad4a0, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1155)...: 
MPIC_Isend(511).............: 
MPID_Isend_coll(610)........: 
MPIDI_isend_coll_unsafe(176): 
MPIDI_OFI_send_normal(352)..: OFI tagged senddata failed (ofi_send.h:352:MPIDI_OFI_send_normal:No route to host)
MPICH ERROR [Rank 15] [job id b711ba51-b89f-471f-b8c7-7a2d81744b0c] [Sun Nov 12 08:56:03 2023] [x3001c0s1b1n0] - Abort(137435791) (rank 15 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14aa78000000, scnts=0x1c84f80, sdispls=0x1c84930, dtype=0x4c000840, rbuf=0x14aa78810000, rcnts=0x1c82f20, rdispls=0x1c45210, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14aa78000000, scnts=0x1c84f80, sdispls=0x1c84930, dtype=0x4c000840, rbuf=0x14aa78810000, rcnts=0x1c82f20, rdispls=0x1c45210, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 0] [job id b711ba51-b89f-471f-b8c7-7a2d81744b0c] [Sun Nov 12 08:56:03 2023] [x3001c0s19b0n0] - Abort(942742159) (rank 0 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14b798000000, scnts=0x27566b0, sdispls=0x27734e0, dtype=0x4c000840, rbuf=0x14b798810000, rcnts=0x2773600, rdispls=0x2773690, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14b798000000, scnts=0x27566b0, sdispls=0x27734e0, dtype=0x4c000840, rbuf=0x14b798810000, rcnts=0x2773600, rdispls=0x2773690, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 23] [job id b711ba51-b89f-471f-b8c7-7a2d81744b0c] [Sun Nov 12 08:56:03 2023] [x3001c0s25b1n0] - Abort(204544655) (rank 23 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14ede8000000, scnts=0x7b7b70, sdispls=0x799930, dtype=0x4c000840, rbuf=0x14ede8800000, rcnts=0x796610, rdispls=0x7966a0, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14ede8000000, scnts=0x7b7b70, sdispls=0x799930, dtype=0x4c000840, rbuf=0x14ede8800000, rcnts=0x796610, rdispls=0x7966a0, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)
MPICH ERROR [Rank 17] [job id b711ba51-b89f-471f-b8c7-7a2d81744b0c] [Sun Nov 12 08:56:03 2023] [x3001c0s25b0n0] - Abort(338237967) (rank 17 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389).........: MPI_Alltoallv(sbuf=0x15484c000000, scnts=0xbcd670, sdispls=0xb93710, dtype=0x4c000840, rbuf=0x15484c800000, rcnts=0xbd3ed0, rdispls=0xbcdcd0, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1155)...: 
MPIC_Isend(511).............: 
MPID_Isend_coll(610)........: 
MPIDI_isend_coll_unsafe(176): 
MPIDI_OFI_send_normal(352)..: OFI tagged senddata failed (ofi_send.h:352:MPIDI_OFI_send_normal:No route to host)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389).........: MPI_Alltoallv(sbuf=0x15484c000000, scnts=0xbcd670, sdispls=0xb93710, dtype=0x4c000840, rbuf=0x15484c800000, rcnts=0xbd3ed0, rdispls=0xbcdcd0, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1155)...: 
MPIC_Isend(511).............: 
MPID_Isend_coll(610)........: 
MPIDI_isend_coll_unsafe(176): 
MPIDI_OFI_send_normal(352)..: OFI tagged senddata failed (ofi_send.h:352:MPIDI_OFI_send_normal:No route to host)
MPICH ERROR [Rank 7] [job id b711ba51-b89f-471f-b8c7-7a2d81744b0c] [Sun Nov 12 08:56:03 2023] [x3001c0s19b1n0] - Abort(405871247) (rank 7 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x146722000000, scnts=0x21ce060, sdispls=0x2194ec0, dtype=0x4c000840, rbuf=0x146722810000, rcnts=0x21ce6c0, rdispls=0x21ce750, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x146722000000, scnts=0x21ce060, sdispls=0x2194ec0, dtype=0x4c000840, rbuf=0x146722810000, rcnts=0x21ce6c0, rdispls=0x21ce750, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 25] [job id b711ba51-b89f-471f-b8c7-7a2d81744b0c] [Sun Nov 12 08:56:03 2023] [x3001c0s31b0n0] - Abort(875633295) (rank 25 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14ea02000000, scnts=0x244c2f0, sdispls=0x244cba0, dtype=0x4c000840, rbuf=0x14ea02800000, rcnts=0x244ccc0, rdispls=0x244cd50, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14ea02000000, scnts=0x244c2f0, sdispls=0x244cba0, dtype=0x4c000840, rbuf=0x14ea02800000, rcnts=0x244ccc0, rdispls=0x244cd50, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)
MPICH ERROR [Rank 26] [job id b711ba51-b89f-471f-b8c7-7a2d81744b0c] [Sun Nov 12 08:56:03 2023] [x3001c0s31b0n0] - Abort(540088975) (rank 26 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x146462000000, scnts=0x27e49a0, sdispls=0x26a5930, dtype=0x4c000840, rbuf=0x146462800000, rcnts=0x26844f0, rdispls=0x2684580, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x146462000000, scnts=0x27e49a0, sdispls=0x26a5930, dtype=0x4c000840, rbuf=0x146462800000, rcnts=0x26844f0, rdispls=0x2684580, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)
x3001c0s19b1n0.hsn.cm.polaris.alcf.anl.gov: rank 4 exited with code 255
MPICH ERROR [Rank 5] [job id b711ba51-b89f-471f-b8c7-7a2d81744b0c] [Sun Nov 12 08:56:03 2023] [x3001c0s19b1n0] - Abort(808524431) (rank 5 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x148a22000000, scnts=0x1f702f0, sdispls=0x1f70ba0, dtype=0x4c000840, rbuf=0x148a22810000, rcnts=0x1f70cc0, rdispls=0x1f70d50, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x148a22000000, scnts=0x1f702f0, sdispls=0x1f70ba0, dtype=0x4c000840, rbuf=0x148a22810000, rcnts=0x1f70cc0, rdispls=0x1f70d50, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 20] [job id b711ba51-b89f-471f-b8c7-7a2d81744b0c] [Sun Nov 12 08:56:03 2023] [x3001c0s25b1n0] - Abort(1009851023) (rank 20 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14fef2000000, scnts=0x9e1940, sdispls=0x9fc6b0, dtype=0x4c000840, rbuf=0x14fef2800000, rcnts=0x9fc7d0, rdispls=0x9fc860, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14fef2000000, scnts=0x9e1940, sdispls=0x9fc6b0, dtype=0x4c000840, rbuf=0x14fef2800000, rcnts=0x9fc7d0, rdispls=0x9fc860, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)
MPICH ERROR [Rank 8] [job id b711ba51-b89f-471f-b8c7-7a2d81744b0c] [Sun Nov 12 08:56:03 2023] [x3001c0s1b0n0] - Abort(875108879) (rank 8 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389).........: MPI_Alltoallv(sbuf=0x14a3aa000000, scnts=0x1214c50, sdispls=0x122daf0, dtype=0x4c000840, rbuf=0x14a3aa810000, rcnts=0x122dc10, rdispls=0x122dca0, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1155)...: 
MPIC_Isend(511).............: 
MPID_Isend_coll(610)........: 
MPIDI_isend_coll_unsafe(176): 
MPIDI_OFI_send_normal(352)..: OFI tagged senddata failed (ofi_send.h:352:MPIDI_OFI_send_normal:No route to host)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389).........: MPI_Alltoallv(sbuf=0x14a3aa000000, scnts=0x1214c50, sdispls=0x122daf0, dtype=0x4c000840, rbuf=0x14a3aa810000, rcnts=0x122dc10, rdispls=0x122dca0, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1155)...: 
MPIC_Isend(511).............: 
MPID_Isend_coll(610)........: 
MPIDI_isend_coll_unsafe(176): 
MPIDI_OFI_send_normal(352)..: OFI tagged senddata failed (ofi_send.h:352:MPIDI_OFI_send_normal:No route to host)
