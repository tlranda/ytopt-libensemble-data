“RANK= 6 LOCAL_RANK= 2 gpu= 1”
“RANK= 7 LOCAL_RANK= 3 gpu= 0”
“RANK= 5 LOCAL_RANK= 1 gpu= 2”
“RANK= 4 LOCAL_RANK= 0 gpu= 3”
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ speed3d_r2c cufft float 256 256 256 -no-reorder -r2c_dir 1 -ingrid 4 4 1
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 256 256 -no-reorder -r2c_dir 1 -ingrid 4 4 1
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 256 256 -no-reorder -r2c_dir 1 -ingrid 4 4 1
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 256 256 -no-reorder -r2c_dir 1 -ingrid 4 4 1
“RANK= 10 LOCAL_RANK= 2 gpu= 1”
“RANK= 8 LOCAL_RANK= 0 gpu= 3”
“RANK= 9 LOCAL_RANK= 1 gpu= 2”
“RANK= 11 LOCAL_RANK= 3 gpu= 0”
“RANK= 15 LOCAL_RANK= 3 gpu= 0”
“RANK= 12 LOCAL_RANK= 0 gpu= 3”
“RANK= 14 LOCAL_RANK= 2 gpu= 1”
“RANK= 13 LOCAL_RANK= 1 gpu= 2”
+ '[' -z '' ']'
+ case "$-" in
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ unset __lmod_vx
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 256 256 -no-reorder -r2c_dir 1 -ingrid 4 4 1
Shell debugging restarted
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 256 256 -no-reorder -r2c_dir 1 -ingrid 4 4 1
+ speed3d_r2c cufft float 256 256 256 -no-reorder -r2c_dir 1 -ingrid 4 4 1
+ speed3d_r2c cufft float 256 256 256 -no-reorder -r2c_dir 1 -ingrid 4 4 1
+ speed3d_r2c cufft float 256 256 256 -no-reorder -r2c_dir 1 -ingrid 4 4 1
+ speed3d_r2c cufft float 256 256 256 -no-reorder -r2c_dir 1 -ingrid 4 4 1
+ speed3d_r2c cufft float 256 256 256 -no-reorder -r2c_dir 1 -ingrid 4 4 1
+ speed3d_r2c cufft float 256 256 256 -no-reorder -r2c_dir 1 -ingrid 4 4 1
“RANK= 3 LOCAL_RANK= 3 gpu= 0”
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 256 256 -no-reorder -r2c_dir 1 -ingrid 4 4 1
“RANK= 2 LOCAL_RANK= 2 gpu= 1”
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 256 256 -no-reorder -r2c_dir 1 -ingrid 4 4 1
“RANK= 0 LOCAL_RANK= 0 gpu= 3”
“RANK= 1 LOCAL_RANK= 1 gpu= 2”
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 256 256 -no-reorder -r2c_dir 1 -ingrid 4 4 1
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 256 256 -no-reorder -r2c_dir 1 -ingrid 4 4 1
MPICH ERROR [Rank 7] [job id c5077831-00b4-4dea-b8bc-6afdd004d939] [Sat Jul  1 22:43:13 2023] [x3010c0s37b1n0] - Abort(875633295) (rank 7 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x15038c000000, scnts=0x1df7c50, sdispls=0x1df7f30, dtype=0x4c000840, rbuf=0x15038c400000, rcnts=0x1df8390, rdispls=0x1df83c0, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x15038c000000, scnts=0x1df7c50, sdispls=0x1df7f30, dtype=0x4c000840, rbuf=0x15038c400000, rcnts=0x1df8390, rdispls=0x1df83c0, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 6] [job id c5077831-00b4-4dea-b8bc-6afdd004d939] [Sat Jul  1 22:43:13 2023] [x3010c0s37b1n0] - Abort(942742159) (rank 6 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x154edc000000, scnts=0x1a49ee0, sdispls=0x1a4a1c0, dtype=0x4c000840, rbuf=0x154edc400000, rcnts=0x1a4a8b0, rdispls=0x1a4a8e0, datatype=dtype=0x4c000840, comm=comm=0xc4000004) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x154edc000000, scnts=0x1a49ee0, sdispls=0x1a4a1c0, dtype=0x4c000840, rbuf=0x154edc400000, rcnts=0x1a4a8b0, rdispls=0x1a4a8e0, datatype=dtype=0x4c000840, comm=comm=0xc4000004) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 2] [job id c5077831-00b4-4dea-b8bc-6afdd004d939] [Sat Jul  1 22:43:13 2023] [x3010c0s37b0n0] - Abort(338762383) (rank 2 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14d18c000000, scnts=0x249e800, sdispls=0x24efbb0, dtype=0x4c000840, rbuf=0x14d18c400000, rcnts=0x24f02a0, rdispls=0x24f02d0, datatype=dtype=0x4c000840, comm=comm=0xc4000004) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14d18c000000, scnts=0x249e800, sdispls=0x24efbb0, dtype=0x4c000840, rbuf=0x14d18c400000, rcnts=0x24f02a0, rdispls=0x24f02d0, datatype=dtype=0x4c000840, comm=comm=0xc4000004) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 3] [job id c5077831-00b4-4dea-b8bc-6afdd004d939] [Sat Jul  1 22:43:13 2023] [x3010c0s37b0n0] - Abort(808524431) (rank 3 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x154e2c000000, scnts=0xa61800, sdispls=0xab2920, dtype=0x4c000840, rbuf=0x154e2c400000, rcnts=0xab2d80, rdispls=0xab2db0, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x154e2c000000, scnts=0xa61800, sdispls=0xab2920, dtype=0x4c000840, rbuf=0x154e2c400000, rcnts=0xab2d80, rdispls=0xab2db0, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)
MPICH ERROR [Rank 10] [job id c5077831-00b4-4dea-b8bc-6afdd004d939] [Sat Jul  1 22:43:13 2023] [x3010c0s7b0n0] - Abort(607197839) (rank 10 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14570a000000, scnts=0x12f1690, sdispls=0x12f1970, dtype=0x4c000840, rbuf=0x14570a400000, rcnts=0x130e460, rdispls=0x130e490, datatype=dtype=0x4c000840, comm=comm=0xc4000004) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14570a000000, scnts=0x12f1690, sdispls=0x12f1970, dtype=0x4c000840, rbuf=0x14570a400000, rcnts=0x130e460, rdispls=0x130e490, datatype=dtype=0x4c000840, comm=comm=0xc4000004) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 11] [job id c5077831-00b4-4dea-b8bc-6afdd004d939] [Sat Jul  1 22:43:13 2023] [x3010c0s7b0n0] - Abort(674306703) (rank 11 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14fdec000000, scnts=0x189a3d0, sdispls=0x189a6b0, dtype=0x4c000840, rbuf=0x14fdec400000, rcnts=0x189ab10, rdispls=0x189ab40, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14fdec000000, scnts=0x189a3d0, sdispls=0x189a6b0, dtype=0x4c000840, rbuf=0x14fdec400000, rcnts=0x189ab10, rdispls=0x189ab40, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)
MPICH ERROR [Rank 15] [job id c5077831-00b4-4dea-b8bc-6afdd004d939] [Sat Jul  1 22:43:13 2023] [x3010c0s7b1n0] - Abort(1009851023) (rank 15 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14763c000000, scnts=0x231df90, sdispls=0x22fcf10, dtype=0x4c000840, rbuf=0x14763c400000, rcnts=0x22fd370, rdispls=0x22fd3a0, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14763c000000, scnts=0x231df90, sdispls=0x22fcf10, dtype=0x4c000840, rbuf=0x14763c400000, rcnts=0x22fd370, rdispls=0x22fd3a0, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 14] [job id c5077831-00b4-4dea-b8bc-6afdd004d939] [Sat Jul  1 22:43:13 2023] [x3010c0s7b1n0] - Abort(204544655) (rank 14 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14f14c000000, scnts=0x163f180, sdispls=0x163f460, dtype=0x4c000840, rbuf=0x14f14c400000, rcnts=0x163fb50, rdispls=0x163fb80, datatype=dtype=0x4c000840, comm=comm=0xc4000004) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14f14c000000, scnts=0x163f180, sdispls=0x163f460, dtype=0x4c000840, rbuf=0x14f14c400000, rcnts=0x163fb50, rdispls=0x163fb80, datatype=dtype=0x4c000840, comm=comm=0xc4000004) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 9] [job id c5077831-00b4-4dea-b8bc-6afdd004d939] [Sat Jul  1 22:43:13 2023] [x3010c0s7b0n0] - Abort(204544655) (rank 9 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14eacc000000, scnts=0x19c73d0, sdispls=0x19c76b0, dtype=0x4c000840, rbuf=0x14eacc410000, rcnts=0x19c7b10, rdispls=0x19c7b40, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14eacc000000, scnts=0x19c73d0, sdispls=0x19c76b0, dtype=0x4c000840, rbuf=0x14eacc410000, rcnts=0x19c7b10, rdispls=0x19c7b40, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)
x3010c0s7b1n0.hsn.cm.polaris.alcf.anl.gov: rank 15 exited with code 255
MPICH ERROR [Rank 0] [job id c5077831-00b4-4dea-b8bc-6afdd004d939] [Sat Jul  1 22:43:13 2023] [x3010c0s37b0n0] - Abort(271653519) (rank 0 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x145732000000, scnts=0x108afb0, sdispls=0x10e2090, dtype=0x4c000840, rbuf=0x145732410000, rcnts=0x10e2780, rdispls=0x10e27b0, datatype=dtype=0x4c000840, comm=comm=0xc4000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x145732000000, scnts=0x108afb0, sdispls=0x10e2090, dtype=0x4c000840, rbuf=0x145732410000, rcnts=0x10e2780, rdispls=0x10e27b0, datatype=dtype=0x4c000840, comm=comm=0xc4000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 5] [job id c5077831-00b4-4dea-b8bc-6afdd004d939] [Sat Jul  1 22:43:13 2023] [x3010c0s37b1n0] - Abort(271653519) (rank 5 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x145b48000000, scnts=0x979890, sdispls=0x9cade0, dtype=0x4c000840, rbuf=0x145b48410000, rcnts=0x9cb240, rdispls=0x9cb270, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x145b48000000, scnts=0x979890, sdispls=0x9cade0, dtype=0x4c000840, rbuf=0x145b48410000, rcnts=0x9cb240, rdispls=0x9cb270, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)
MPICH ERROR [Rank 1] [job id c5077831-00b4-4dea-b8bc-6afdd004d939] [Sat Jul  1 22:43:13 2023] [x3010c0s37b0n0] - Abort(3218063) (rank 1 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x148f7c000000, scnts=0x830fe0, sdispls=0x82d580, dtype=0x4c000840, rbuf=0x148f7c410000, rcnts=0x82d9e0, rdispls=0x82da10, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x148f7c000000, scnts=0x830fe0, sdispls=0x82d580, dtype=0x4c000840, rbuf=0x148f7c410000, rcnts=0x82d9e0, rdispls=0x82da10, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
