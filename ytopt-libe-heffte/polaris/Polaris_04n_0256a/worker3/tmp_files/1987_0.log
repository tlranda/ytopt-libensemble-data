“RANK= 0 LOCAL_RANK= 0 gpu= 3”
“RANK= 12 LOCAL_RANK= 0 gpu= 3”
“RANK= 15 LOCAL_RANK= 3 gpu= 0”
“RANK= 1 LOCAL_RANK= 1 gpu= 2”
“RANK= 13 LOCAL_RANK= 1 gpu= 2”
“RANK= 2 LOCAL_RANK= 2 gpu= 1”
“RANK= 14 LOCAL_RANK= 2 gpu= 1”
“RANK= 3 LOCAL_RANK= 3 gpu= 0”
+ '[' -z '' ']'
+ case "$-" in
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
“RANK= 8 LOCAL_RANK= 0 gpu= 3”
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ unset __lmod_vx
“RANK= 9 LOCAL_RANK= 1 gpu= 2”
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
“RANK= 10 LOCAL_RANK= 2 gpu= 1”
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
“RANK= 4 LOCAL_RANK= 0 gpu= 3”
“RANK= 11 LOCAL_RANK= 3 gpu= 0”
Shell debugging restarted
+ unset __lmod_vx
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
“RANK= 5 LOCAL_RANK= 1 gpu= 2”
Shell debugging restarted
+ unset __lmod_vx
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 256 256 256 -no-reorder -p2p -pencils -r2c_dir 2 -outgrid 4 4 1 -n5
+ speed3d_r2c cufft double 256 256 256 -no-reorder -p2p -pencils -r2c_dir 2 -outgrid 4 4 1 -n5
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 256 256 256 -no-reorder -p2p -pencils -r2c_dir 2 -outgrid 4 4 1 -n5
Shell debugging restarted
+ unset __lmod_vx
“RANK= 6 LOCAL_RANK= 2 gpu= 1”
+ speed3d_r2c cufft double 256 256 256 -no-reorder -p2p -pencils -r2c_dir 2 -outgrid 4 4 1 -n5
+ speed3d_r2c cufft double 256 256 256 -no-reorder -p2p -pencils -r2c_dir 2 -outgrid 4 4 1 -n5
“RANK= 7 LOCAL_RANK= 3 gpu= 0”
+ speed3d_r2c cufft double 256 256 256 -no-reorder -p2p -pencils -r2c_dir 2 -outgrid 4 4 1 -n5
+ speed3d_r2c cufft double 256 256 256 -no-reorder -p2p -pencils -r2c_dir 2 -outgrid 4 4 1 -n5
+ speed3d_r2c cufft double 256 256 256 -no-reorder -p2p -pencils -r2c_dir 2 -outgrid 4 4 1 -n5
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -n x ']'
+ set +x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ unset __lmod_vx
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ speed3d_r2c cufft double 256 256 256 -no-reorder -p2p -pencils -r2c_dir 2 -outgrid 4 4 1 -n5
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ speed3d_r2c cufft double 256 256 256 -no-reorder -p2p -pencils -r2c_dir 2 -outgrid 4 4 1 -n5
Shell debugging restarted
+ speed3d_r2c cufft double 256 256 256 -no-reorder -p2p -pencils -r2c_dir 2 -outgrid 4 4 1 -n5
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 256 256 256 -no-reorder -p2p -pencils -r2c_dir 2 -outgrid 4 4 1 -n5
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 256 256 256 -no-reorder -p2p -pencils -r2c_dir 2 -outgrid 4 4 1 -n5
+ speed3d_r2c cufft double 256 256 256 -no-reorder -p2p -pencils -r2c_dir 2 -outgrid 4 4 1 -n5
+ speed3d_r2c cufft double 256 256 256 -no-reorder -p2p -pencils -r2c_dir 2 -outgrid 4 4 1 -n5
+ speed3d_r2c cufft double 256 256 256 -no-reorder -p2p -pencils -r2c_dir 2 -outgrid 4 4 1 -n5
MPICH ERROR [Rank 0] [job id d163a0ea-292c-48f3-9887-9354e5813077] [Thu Aug  3 06:37:15 2023] [x3208c0s13b1n0] - Abort(338286991) (rank 0 in comm 0): Fatal error in PMPI_Send: Other MPI error, error stack:
PMPI_Send(163).................: MPI_Send(buf=0x14dc56000000, count=131072, dtype=0x4c001041, dest=1, tag=0, MPI_COMM_WORLD) failed
PMPI_Send(143).................: 
MPIR_Wait_impl(41).............: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)

aborting job:
Fatal error in PMPI_Send: Other MPI error, error stack:
PMPI_Send(163).................: MPI_Send(buf=0x14dc56000000, count=131072, dtype=0x4c001041, dest=1, tag=0, MPI_COMM_WORLD) failed
PMPI_Send(143).................: 
MPIR_Wait_impl(41).............: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)
MPICH ERROR [Rank 2] [job id d163a0ea-292c-48f3-9887-9354e5813077] [Thu Aug  3 06:37:15 2023] [x3208c0s13b1n0] - Abort(405395855) (rank 2 in comm 0): Fatal error in PMPI_Send: Other MPI error, error stack:
PMPI_Send(163).................: MPI_Send(buf=0x1467a4000000, count=135168, dtype=0x4c001041, dest=8, tag=0, MPI_COMM_WORLD) failed
PMPI_Send(143).................: 
MPIR_Wait_impl(41).............: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)

aborting job:
Fatal error in PMPI_Send: Other MPI error, error stack:
PMPI_Send(163).................: MPI_Send(buf=0x1467a4000000, count=135168, dtype=0x4c001041, dest=8, tag=0, MPI_COMM_WORLD) failed
PMPI_Send(143).................: 
MPIR_Wait_impl(41).............: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)
MPICH ERROR [Rank 8] [job id d163a0ea-292c-48f3-9887-9354e5813077] [Thu Aug  3 06:37:15 2023] [x3208c0s1b0n0] - Abort(69851535) (rank 8 in comm 0): Fatal error in PMPI_Send: Other MPI error, error stack:
PMPI_Send(163).................: MPI_Send(buf=0x147d46000000, count=135168, dtype=0x4c001041, dest=0, tag=0, MPI_COMM_WORLD) failed
PMPI_Send(143).................: 
MPIR_Wait_impl(41).............: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Send: Other MPI error, error stack:
PMPI_Send(163).................: MPI_Send(buf=0x147d46000000, count=135168, dtype=0x4c001041, dest=0, tag=0, MPI_COMM_WORLD) failed
PMPI_Send(143).................: 
MPIR_Wait_impl(41).............: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 12] [job id d163a0ea-292c-48f3-9887-9354e5813077] [Thu Aug  3 06:37:15 2023] [x3208c0s1b1n0] - Abort(606722447) (rank 12 in comm 0): Fatal error in PMPI_Send: Other MPI error, error stack:
PMPI_Send(163).................: MPI_Send(buf=0x153da4000000, count=135168, dtype=0x4c001041, dest=0, tag=0, MPI_COMM_WORLD) failed
PMPI_Send(143).................: 
MPIR_Wait_impl(41).............: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Send: Other MPI error, error stack:
PMPI_Send(163).................: MPI_Send(buf=0x153da4000000, count=135168, dtype=0x4c001041, dest=0, tag=0, MPI_COMM_WORLD) failed
PMPI_Send(143).................: 
MPIR_Wait_impl(41).............: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 10] [job id d163a0ea-292c-48f3-9887-9354e5813077] [Thu Aug  3 06:37:15 2023] [x3208c0s1b0n0] - Abort(606722447) (rank 10 in comm 0): Fatal error in PMPI_Send: Other MPI error, error stack:
PMPI_Send(163).................: MPI_Send(buf=0x14efc6000000, count=131072, dtype=0x4c001041, dest=12, tag=0, MPI_COMM_WORLD) failed
PMPI_Send(143).................: 
MPIR_Wait_impl(41).............: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)

aborting job:
Fatal error in PMPI_Send: Other MPI error, error stack:
PMPI_Send(163).................: MPI_Send(buf=0x14efc6000000, count=131072, dtype=0x4c001041, dest=12, tag=0, MPI_COMM_WORLD) failed
PMPI_Send(143).................: 
MPIR_Wait_impl(41).............: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)
MPICH ERROR [Rank 14] [job id d163a0ea-292c-48f3-9887-9354e5813077] [Thu Aug  3 06:37:15 2023] [x3208c0s1b1n0] - Abort(472504719) (rank 14 in comm 0): Fatal error in PMPI_Send: Other MPI error, error stack:
PMPI_Send(163).................: MPI_Send(buf=0x154c56000000, count=135168, dtype=0x4c001041, dest=8, tag=0, MPI_COMM_WORLD) failed
PMPI_Send(143).................: 
MPIR_Wait_impl(41).............: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Send: Other MPI error, error stack:
PMPI_Send(163).................: MPI_Send(buf=0x154c56000000, count=135168, dtype=0x4c001041, dest=8, tag=0, MPI_COMM_WORLD) failed
PMPI_Send(143).................: 
MPIR_Wait_impl(41).............: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 4] [job id d163a0ea-292c-48f3-9887-9354e5813077] [Thu Aug  3 06:37:15 2023] [x3208c0s19b0n0] - Abort(68802831) (rank 4 in comm 0): Fatal error in PMPI_Send: Other MPI error, error stack:
PMPI_Send(163)............: MPI_Send(buf=0x14e4d6200000, count=135168, dtype=0x4c001041, dest=0, tag=0, MPI_COMM_WORLD) failed
MPID_Send(499)............: 
MPIDI_send_unsafe(58).....: 
MPIDI_OFI_send_normal(352): OFI tagged senddata failed (ofi_send.h:352:MPIDI_OFI_send_normal:No route to host)

aborting job:
Fatal error in PMPI_Send: Other MPI error, error stack:
PMPI_Send(163)............: MPI_Send(buf=0x14e4d6200000, count=135168, dtype=0x4c001041, dest=0, tag=0, MPI_COMM_WORLD) failed
MPID_Send(499)............: 
MPIDI_send_unsafe(58).....: 
MPIDI_OFI_send_normal(352): OFI tagged senddata failed (ofi_send.h:352:MPIDI_OFI_send_normal:No route to host)
MPICH ERROR [Rank 5] [job id d163a0ea-292c-48f3-9887-9354e5813077] [Thu Aug  3 06:37:15 2023] [x3208c0s19b0n0] - Abort(135911695) (rank 5 in comm 0): Fatal error in PMPI_Send: Other MPI error, error stack:
PMPI_Send(163)............: MPI_Send(buf=0x152346400000, count=135168, dtype=0x4c001041, dest=2, tag=0, MPI_COMM_WORLD) failed
MPID_Send(499)............: 
MPIDI_send_unsafe(58).....: 
MPIDI_OFI_send_normal(352): OFI tagged senddata failed (ofi_send.h:352:MPIDI_OFI_send_normal:No route to host)

aborting job:
Fatal error in PMPI_Send: Other MPI error, error stack:
PMPI_Send(163)............: MPI_Send(buf=0x152346400000, count=135168, dtype=0x4c001041, dest=2, tag=0, MPI_COMM_WORLD) failed
MPID_Send(499)............: 
MPIDI_send_unsafe(58).....: 
MPIDI_OFI_send_normal(352): OFI tagged senddata failed (ofi_send.h:352:MPIDI_OFI_send_normal:No route to host)
x3208c0s1b0n0.hsn.cm.polaris.alcf.anl.gov: rank 8 exited with code 255
(GTL DEBUG: 1) cuIpcOpenMemHandle: invalid resource handle, CUDA_ERROR_INVALID_HANDLE, line no 272
MPICH ERROR [Rank 1] [job id d163a0ea-292c-48f3-9887-9354e5813077] [Thu Aug  3 06:37:15 2023] [x3208c0s13b1n0] - Abort(406431234) (rank 1 in comm 0): Fatal error in PMPI_Irecv: Invalid count, error stack:
PMPI_Irecv(166)......................: MPI_Irecv(buf=0x1541e2e10000, count=131072, dtype=0x4c001041, src=0, tag=0, MPI_COMM_WORLD, request=0x11027bc) failed
MPID_Irecv(497)......................: 
MPIDI_irecv_unsafe(160)..............: 
MPIDI_SHM_mpi_irecv(462).............: 
MPIDI_SHM_mpi_imrecv(514)............: 
MPIDI_SHM_mmods_try_matched_recv(167): 
MPIDI_CRAY_Common_lmt_handle_recv(44): 
MPIDI_CRAY_Common_lmt_import_mem(218): 
(unknown)(): Invalid count

aborting job:
Fatal error in PMPI_Irecv: Invalid count, error stack:
PMPI_Irecv(166)......................: MPI_Irecv(buf=0x1541e2e10000, count=131072, dtype=0x4c001041, src=0, tag=0, MPI_COMM_WORLD, request=0x11027bc) failed
MPID_Irecv(497)......................: 
MPIDI_irecv_unsafe(160)..............: 
MPIDI_SHM_mpi_irecv(462).............: 
MPIDI_SHM_mpi_imrecv(514)............: 
MPIDI_SHM_mmods_try_matched_recv(167): 
MPIDI_CRAY_Common_lmt_handle_recv(44): 
MPIDI_CRAY_Common_lmt_import_mem(218): 
(unknown)(): Invalid count
