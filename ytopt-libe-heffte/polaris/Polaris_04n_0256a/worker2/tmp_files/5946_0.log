“RANK= 8 LOCAL_RANK= 0 gpu= 3”
“RANK= 4 LOCAL_RANK= 0 gpu= 3”
“RANK= 6 LOCAL_RANK= 2 gpu= 1”
“RANK= 10 LOCAL_RANK= 2 gpu= 1”
“RANK= 5 LOCAL_RANK= 1 gpu= 2”
“RANK= 9 LOCAL_RANK= 1 gpu= 2”
“RANK= 12 LOCAL_RANK= 0 gpu= 3”
“RANK= 11 LOCAL_RANK= 3 gpu= 0”
“RANK= 7 LOCAL_RANK= 3 gpu= 0”
“RANK= 13 LOCAL_RANK= 1 gpu= 2”
“RANK= 15 LOCAL_RANK= 3 gpu= 0”
“RANK= 14 LOCAL_RANK= 2 gpu= 1”
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -z '' ']'
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
Shell debugging restarted
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ unset __lmod_vx
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -pencils -r2c_dir 1 -ingrid 4 2 2 -outgrid 8 2 1 -n5
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -pencils -r2c_dir 1 -ingrid 4 2 2 -outgrid 8 2 1 -n5
Shell debugging restarted
+ unset __lmod_vx
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -pencils -r2c_dir 1 -ingrid 4 2 2 -outgrid 8 2 1 -n5
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -pencils -r2c_dir 1 -ingrid 4 2 2 -outgrid 8 2 1 -n5
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -pencils -r2c_dir 1 -ingrid 4 2 2 -outgrid 8 2 1 -n5
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
Shell debugging restarted
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -pencils -r2c_dir 1 -ingrid 4 2 2 -outgrid 8 2 1 -n5
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -pencils -r2c_dir 1 -ingrid 4 2 2 -outgrid 8 2 1 -n5
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -pencils -r2c_dir 1 -ingrid 4 2 2 -outgrid 8 2 1 -n5
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -pencils -r2c_dir 1 -ingrid 4 2 2 -outgrid 8 2 1 -n5
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -pencils -r2c_dir 1 -ingrid 4 2 2 -outgrid 8 2 1 -n5
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -pencils -r2c_dir 1 -ingrid 4 2 2 -outgrid 8 2 1 -n5
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -pencils -r2c_dir 1 -ingrid 4 2 2 -outgrid 8 2 1 -n5
“RANK= 2 LOCAL_RANK= 2 gpu= 1”
“RANK= 1 LOCAL_RANK= 1 gpu= 2”
“RANK= 0 LOCAL_RANK= 0 gpu= 3”
“RANK= 3 LOCAL_RANK= 3 gpu= 0”
+ '[' -z '' ']'
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -pencils -r2c_dir 1 -ingrid 4 2 2 -outgrid 8 2 1 -n5
Shell debugging restarted
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -pencils -r2c_dir 1 -ingrid 4 2 2 -outgrid 8 2 1 -n5
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -pencils -r2c_dir 1 -ingrid 4 2 2 -outgrid 8 2 1 -n5
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -pencils -r2c_dir 1 -ingrid 4 2 2 -outgrid 8 2 1 -n5
MPICH ERROR [Rank 7] [job id 76c3ac31-ab95-41c6-a5ba-03f362df66cf] [Thu Aug  3 06:35:53 2023] [x3206c0s7b0n0] - Abort(338762127) (rank 7 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x147b48000000, scount=67584, dtype=0x4c000840, rbuf=0x147b48840000, rcount=67584, datatype=dtype=0x4c000840, comm=comm=0xc4000001) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x147b48000000, scount=67584, dtype=0x4c000840, rbuf=0x147b48840000, rcount=67584, datatype=dtype=0x4c000840, comm=comm=0xc4000001) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 8] [job id 76c3ac31-ab95-41c6-a5ba-03f362df66cf] [Thu Aug  3 06:35:53 2023] [x3206c0s7b1n0] - Abort(1009850767) (rank 8 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x151efc000000, scount=67584, dtype=0x4c000840, rbuf=0x151efc840000, rcount=67584, datatype=dtype=0x4c000840, comm=comm=0xc4000005) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x151efc000000, scount=67584, dtype=0x4c000840, rbuf=0x151efc840000, rcount=67584, datatype=dtype=0x4c000840, comm=comm=0xc4000005) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 2] [job id 76c3ac31-ab95-41c6-a5ba-03f362df66cf] [Thu Aug  3 06:35:53 2023] [x3206c0s37b1n0] - Abort(607197583) (rank 2 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x14f8fc000000, scount=67584, dtype=0x4c000840, rbuf=0x14f8fc840000, rcount=67584, datatype=dtype=0x4c000840, comm=comm=0xc4000001) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x14f8fc000000, scount=67584, dtype=0x4c000840, rbuf=0x14f8fc840000, rcount=67584, datatype=dtype=0x4c000840, comm=comm=0xc4000001) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 3] [job id 76c3ac31-ab95-41c6-a5ba-03f362df66cf] [Thu Aug  3 06:35:53 2023] [x3206c0s37b1n0] - Abort(875633039) (rank 3 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x14ccc8000000, scount=67584, dtype=0x4c000840, rbuf=0x14ccc8840000, rcount=67584, datatype=dtype=0x4c000840, comm=comm=0xc4000001) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x14ccc8000000, scount=67584, dtype=0x4c000840, rbuf=0x14ccc8840000, rcount=67584, datatype=dtype=0x4c000840, comm=comm=0xc4000001) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 12] [job id 76c3ac31-ab95-41c6-a5ba-03f362df66cf] [Thu Aug  3 06:35:53 2023] [x3207c0s13b0n0] - Abort(741415311) (rank 12 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x1501ca000000, scount=67584, dtype=0x4c000840, rbuf=0x1501ca840000, rcount=67584, datatype=dtype=0x4c000840, comm=comm=0xc4000005) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x1501ca000000, scount=67584, dtype=0x4c000840, rbuf=0x1501ca840000, rcount=67584, datatype=dtype=0x4c000840, comm=comm=0xc4000005) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 1] [job id 76c3ac31-ab95-41c6-a5ba-03f362df66cf] [Thu Aug  3 06:35:53 2023] [x3206c0s37b1n0] - Abort(472979855) (rank 1 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x14a5f2000000, scount=67584, dtype=0x4c000840, rbuf=0x14a5f2840000, rcount=67584, datatype=dtype=0x4c000840, comm=comm=0xc4000001) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x14a5f2000000, scount=67584, dtype=0x4c000840, rbuf=0x14a5f2840000, rcount=67584, datatype=dtype=0x4c000840, comm=comm=0xc4000001) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 15] [job id 76c3ac31-ab95-41c6-a5ba-03f362df66cf] [Thu Aug  3 06:35:53 2023] [x3207c0s13b0n0] - Abort(204544399) (rank 15 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x1476e2000000, scount=67584, dtype=0x4c000840, rbuf=0x1476e2840000, rcount=67584, datatype=dtype=0x4c000840, comm=comm=0xc4000001) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x1476e2000000, scount=67584, dtype=0x4c000840, rbuf=0x1476e2840000, rcount=67584, datatype=dtype=0x4c000840, comm=comm=0xc4000001) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 9] [job id 76c3ac31-ab95-41c6-a5ba-03f362df66cf] [Thu Aug  3 06:35:53 2023] [x3206c0s7b1n0] - Abort(607197583) (rank 9 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x14b0e8000000, scount=67584, dtype=0x4c000840, rbuf=0x14b0e8840000, rcount=67584, datatype=dtype=0x4c000840, comm=comm=0xc4000001) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x14b0e8000000, scount=67584, dtype=0x4c000840, rbuf=0x14b0e8840000, rcount=67584, datatype=dtype=0x4c000840, comm=comm=0xc4000001) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 14] [job id 76c3ac31-ab95-41c6-a5ba-03f362df66cf] [Thu Aug  3 06:35:53 2023] [x3207c0s13b0n0] - Abort(607197583) (rank 14 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x14a8cc000000, scount=67584, dtype=0x4c000840, rbuf=0x14a8cc840000, rcount=67584, datatype=dtype=0x4c000840, comm=comm=0xc4000001) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x14a8cc000000, scount=67584, dtype=0x4c000840, rbuf=0x14a8cc840000, rcount=67584, datatype=dtype=0x4c000840, comm=comm=0xc4000001) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 13] [job id 76c3ac31-ab95-41c6-a5ba-03f362df66cf] [Thu Aug  3 06:35:53 2023] [x3207c0s13b0n0] - Abort(1009850767) (rank 13 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x153fec000000, scount=67584, dtype=0x4c000840, rbuf=0x153fec840000, rcount=67584, datatype=dtype=0x4c000840, comm=comm=0xc4000001) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x153fec000000, scount=67584, dtype=0x4c000840, rbuf=0x153fec840000, rcount=67584, datatype=dtype=0x4c000840, comm=comm=0xc4000001) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 10] [job id 76c3ac31-ab95-41c6-a5ba-03f362df66cf] [Thu Aug  3 06:35:53 2023] [x3206c0s7b1n0] - Abort(338762127) (rank 10 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x1464a2000000, scount=67584, dtype=0x4c000840, rbuf=0x1464a2840000, rcount=67584, datatype=dtype=0x4c000840, comm=comm=0xc4000001) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x1464a2000000, scount=67584, dtype=0x4c000840, rbuf=0x1464a2840000, rcount=67584, datatype=dtype=0x4c000840, comm=comm=0xc4000001) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 11] [job id 76c3ac31-ab95-41c6-a5ba-03f362df66cf] [Thu Aug  3 06:35:53 2023] [x3206c0s7b1n0] - Abort(70326671) (rank 11 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x1516c8000000, scount=67584, dtype=0x4c000840, rbuf=0x1516c8840000, rcount=67584, datatype=dtype=0x4c000840, comm=comm=0xc4000001) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x1516c8000000, scount=67584, dtype=0x4c000840, rbuf=0x1516c8840000, rcount=67584, datatype=dtype=0x4c000840, comm=comm=0xc4000001) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
x3206c0s7b0n0.hsn.cm.polaris.alcf.anl.gov: rank 7 exited with code 255
