“RANK= 8 LOCAL_RANK= 0 gpu= 3”
“RANK= 9 LOCAL_RANK= 1 gpu= 2”
“RANK= 10 LOCAL_RANK= 2 gpu= 1”
“RANK= 11 LOCAL_RANK= 3 gpu= 0”
“RANK= 4 LOCAL_RANK= 0 gpu= 3”
“RANK= 6 LOCAL_RANK= 2 gpu= 1”
“RANK= 7 LOCAL_RANK= 3 gpu= 0”
“RANK= 56 LOCAL_RANK= 0 gpu= 3”
“RANK= 5 LOCAL_RANK= 1 gpu= 2”
“RANK= 16 LOCAL_RANK= 0 gpu= 3”
“RANK= 57 LOCAL_RANK= 1 gpu= 2”
“RANK= 17 LOCAL_RANK= 1 gpu= 2”
“RANK= 58 LOCAL_RANK= 2 gpu= 1”
“RANK= 19 LOCAL_RANK= 3 gpu= 0”
“RANK= 59 LOCAL_RANK= 3 gpu= 0”
“RANK= 18 LOCAL_RANK= 2 gpu= 1”
“RANK= 32 LOCAL_RANK= 0 gpu= 3”
“RANK= 33 LOCAL_RANK= 1 gpu= 2”
“RANK= 52 LOCAL_RANK= 0 gpu= 3”
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
“RANK= 41 LOCAL_RANK= 1 gpu= 2”
“RANK= 40 LOCAL_RANK= 0 gpu= 3”
“RANK= 35 LOCAL_RANK= 3 gpu= 0”
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
“RANK= 34 LOCAL_RANK= 2 gpu= 1”
“RANK= 42 LOCAL_RANK= 2 gpu= 1”
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
“RANK= 54 LOCAL_RANK= 2 gpu= 1”
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
“RANK= 53 LOCAL_RANK= 1 gpu= 2”
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
“RANK= 43 LOCAL_RANK= 3 gpu= 0”
“RANK= 55 LOCAL_RANK= 3 gpu= 0”
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
“RANK= 13 LOCAL_RANK= 1 gpu= 2”
“RANK= 60 LOCAL_RANK= 0 gpu= 3”
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
“RANK= 14 LOCAL_RANK= 2 gpu= 1”
“RANK= 61 LOCAL_RANK= 1 gpu= 2”
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
“RANK= 12 LOCAL_RANK= 0 gpu= 3”
Shell debugging restarted
“RANK= 15 LOCAL_RANK= 3 gpu= 0”
“RANK= 62 LOCAL_RANK= 2 gpu= 1”
+ unset __lmod_vx
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
“RANK= 63 LOCAL_RANK= 3 gpu= 0”
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
Shell debugging restarted
+ unset __lmod_vx
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ '[' -z '' ']'
+ '[' -z '' ']'
+ case "$-" in
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
+ '[' -z '' ']'
+ case "$-" in
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
Shell debugging restarted
+ unset __lmod_vx
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
Shell debugging restarted
+ unset __lmod_vx
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
“RANK= 0 LOCAL_RANK= 0 gpu= 3”
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
“RANK= 1 LOCAL_RANK= 1 gpu= 2”
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ unset __lmod_vx
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ unset __lmod_vx
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
“RANK= 3 LOCAL_RANK= 3 gpu= 0”
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
“RANK= 2 LOCAL_RANK= 2 gpu= 1”
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ unset __lmod_vx
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ set +x
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
“RANK= 38 LOCAL_RANK= 2 gpu= 1”
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
“RANK= 36 LOCAL_RANK= 0 gpu= 3”
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
“RANK= 37 LOCAL_RANK= 1 gpu= 2”
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
“RANK= 44 LOCAL_RANK= 0 gpu= 3”
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
Shell debugging restarted
+ unset __lmod_vx
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
“RANK= 39 LOCAL_RANK= 3 gpu= 0”
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
“RANK= 45 LOCAL_RANK= 1 gpu= 2”
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
+ '[' -z '' ']'
+ case "$-" in
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -n x ']'
+ set +x
“RANK= 47 LOCAL_RANK= 3 gpu= 0”
+ '[' -z '' ']'
+ case "$-" in
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
Shell debugging restarted
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
“RANK= 46 LOCAL_RANK= 2 gpu= 1”
Shell debugging restarted
+ unset __lmod_vx
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ unset __lmod_vx
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
Shell debugging restarted
+ unset __lmod_vx
“RANK= 48 LOCAL_RANK= 0 gpu= 3”
Shell debugging restarted
+ unset __lmod_vx
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
Shell debugging restarted
+ unset __lmod_vx
“RANK= 50 LOCAL_RANK= 2 gpu= 1”
Shell debugging restarted
+ unset __lmod_vx
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
“RANK= 49 LOCAL_RANK= 1 gpu= 2”
Shell debugging restarted
+ unset __lmod_vx
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
“RANK= 51 LOCAL_RANK= 3 gpu= 0”
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
+ unset __lmod_vx
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
Shell debugging restarted
+ unset __lmod_vx
Shell debugging restarted
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
Shell debugging restarted
+ unset __lmod_vx
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
Shell debugging restarted
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
Shell debugging restarted
+ unset __lmod_vx
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ unset __lmod_vx
+ set +x
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
+ unset __lmod_vx
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ unset __lmod_vx
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
+ '[' -n x ']'
+ set +x
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
Shell debugging restarted
+ unset __lmod_vx
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ '[' -z '' ']'
+ case "$-" in
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
Shell debugging restarted
+ unset __lmod_vx
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
Shell debugging restarted
+ unset __lmod_vx
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
“RANK= 20 LOCAL_RANK= 0 gpu= 3”
“RANK= 22 LOCAL_RANK= 2 gpu= 1”
“RANK= 21 LOCAL_RANK= 1 gpu= 2”
“RANK= 23 LOCAL_RANK= 3 gpu= 0”
“RANK= 24 LOCAL_RANK= 0 gpu= 3”
“RANK= 27 LOCAL_RANK= 3 gpu= 0”
“RANK= 25 LOCAL_RANK= 1 gpu= 2”
“RANK= 26 LOCAL_RANK= 2 gpu= 1”
“RANK= 29 LOCAL_RANK= 1 gpu= 2”
“RANK= 31 LOCAL_RANK= 3 gpu= 0”
“RANK= 28 LOCAL_RANK= 0 gpu= 3”
“RANK= 30 LOCAL_RANK= 2 gpu= 1”
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
Shell debugging restarted
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
+ '[' -z '' ']'
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 256 256 256 -slabs -r2c_dir 0 -ingrid 8 8 1 -outgrid 16 2 2 -n5
MPICH ERROR [Rank 13] [job id 7a21707f-a375-4c07-bd14-5a5b62101348] [Fri Aug  4 06:17:48 2023] [x3107c0s37b0n0] - Abort(405871247) (rank 13 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x154644000000, scnts=0x1df2bd0, sdispls=0x1defa70, dtype=0x4c001041, rbuf=0x154644200000, rcnts=0x1defc90, rdispls=0x1defda0, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x154644000000, scnts=0x1df2bd0, sdispls=0x1defa70, dtype=0x4c001041, rbuf=0x154644200000, rcnts=0x1defc90, rdispls=0x1defda0, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 57] [job id 7a21707f-a375-4c07-bd14-5a5b62101348] [Fri Aug  4 06:17:48 2023] [x3108c0s25b1n0] - Abort(875633295) (rank 57 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x1529fa000000, scnts=0x9a2bd0, sdispls=0x99fa70, dtype=0x4c001041, rbuf=0x1529fa200000, rcnts=0x99fc90, rdispls=0x99fda0, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x1529fa000000, scnts=0x9a2bd0, sdispls=0x99fa70, dtype=0x4c001041, rbuf=0x1529fa200000, rcnts=0x99fc90, rdispls=0x99fda0, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)
MPICH ERROR [Rank 11] [job id 7a21707f-a375-4c07-bd14-5a5b62101348] [Fri Aug  4 06:17:48 2023] [x3107c0s31b1n0] - Abort(674306703) (rank 11 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x147fa4000000, scnts=0x2018bd0, sdispls=0x2015a70, dtype=0x4c001041, rbuf=0x147fa4200000, rcnts=0x2015c90, rdispls=0x2015da0, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x147fa4000000, scnts=0x2018bd0, sdispls=0x2015a70, dtype=0x4c001041, rbuf=0x147fa4200000, rcnts=0x2015c90, rdispls=0x2015da0, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 21] [job id 7a21707f-a375-4c07-bd14-5a5b62101348] [Fri Aug  4 06:17:48 2023] [x3107c0s7b0n0] - Abort(942742159) (rank 21 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14a03e000000, scnts=0x1fe0bd0, sdispls=0x1fdda70, dtype=0x4c001041, rbuf=0x14a03e200000, rcnts=0x1fddc90, rdispls=0x1fddda0, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14a03e000000, scnts=0x1fe0bd0, sdispls=0x1fdda70, dtype=0x4c001041, rbuf=0x14a03e200000, rcnts=0x1fddc90, rdispls=0x1fddda0, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 44] [job id 7a21707f-a375-4c07-bd14-5a5b62101348] [Fri Aug  4 06:17:48 2023] [x3108c0s1b0n0] - Abort(3218063) (rank 44 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14d60e000000, scnts=0x21fbcf0, sdispls=0x22eaf80, dtype=0x4c001041, rbuf=0x14d60e200000, rcnts=0x22eb1a0, rdispls=0x22eb2b0, datatype=dtype=0x4c001041, comm=comm=0xc4000009) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14d60e000000, scnts=0x21fbcf0, sdispls=0x22eaf80, dtype=0x4c001041, rbuf=0x14d60e200000, rcnts=0x22eb1a0, rdispls=0x22eb2b0, datatype=dtype=0x4c001041, comm=comm=0xc4000009) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 23] [job id 7a21707f-a375-4c07-bd14-5a5b62101348] [Fri Aug  4 06:17:48 2023] [x3107c0s7b0n0] - Abort(540088975) (rank 23 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x148d32000000, scnts=0x1cc8bd0, sdispls=0x1cc5a70, dtype=0x4c001041, rbuf=0x148d32200000, rcnts=0x1cc5c90, rdispls=0x1cc5da0, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x148d32000000, scnts=0x1cc8bd0, sdispls=0x1cc5a70, dtype=0x4c001041, rbuf=0x148d32200000, rcnts=0x1cc5c90, rdispls=0x1cc5da0, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 31] [job id 7a21707f-a375-4c07-bd14-5a5b62101348] [Fri Aug  4 06:17:48 2023] [x3108c0s13b0n0] - Abort(808524431) (rank 31 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x1544ce000000, scnts=0x1e68bd0, sdispls=0x1e65a70, dtype=0x4c001041, rbuf=0x1544ce200000, rcnts=0x1e65c90, rdispls=0x1e65da0, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x1544ce000000, scnts=0x1e68bd0, sdispls=0x1e65a70, dtype=0x4c001041, rbuf=0x1544ce200000, rcnts=0x1e65c90, rdispls=0x1e65da0, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)
MPICH ERROR [Rank 47] [job id 7a21707f-a375-4c07-bd14-5a5b62101348] [Fri Aug  4 06:17:48 2023] [x3108c0s1b0n0] - Abort(674306703) (rank 47 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14aa1a000000, scnts=0x19e7bd0, sdispls=0x19e4a70, dtype=0x4c001041, rbuf=0x14aa1a200000, rcnts=0x19e4c90, rdispls=0x19e4da0, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14aa1a000000, scnts=0x19e7bd0, sdispls=0x19e4a70, dtype=0x4c001041, rbuf=0x14aa1a200000, rcnts=0x19e4c90, rdispls=0x19e4da0, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 42] [job id 7a21707f-a375-4c07-bd14-5a5b62101348] [Fri Aug  4 06:17:48 2023] [x3108c0s19b1n0] - Abort(204544655) (rank 42 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14f86e000000, scnts=0x166f0a0, sdispls=0x166ff40, dtype=0x4c001041, rbuf=0x14f86e200000, rcnts=0x1670160, rdispls=0x1670270, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14f86e000000, scnts=0x166f0a0, sdispls=0x166ff40, dtype=0x4c001041, rbuf=0x14f86e200000, rcnts=0x1670160, rdispls=0x1670270, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 61] [job id 7a21707f-a375-4c07-bd14-5a5b62101348] [Fri Aug  4 06:17:48 2023] [x3108c0s31b0n0] - Abort(1009851023) (rank 61 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x154992000000, scnts=0x21dcbd0, sdispls=0x21d9a70, dtype=0x4c001041, rbuf=0x154992200000, rcnts=0x21d9c90, rdispls=0x21d9da0, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x154992000000, scnts=0x21dcbd0, sdispls=0x21d9a70, dtype=0x4c001041, rbuf=0x154992200000, rcnts=0x21d9c90, rdispls=0x21d9da0, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 48] [job id 7a21707f-a375-4c07-bd14-5a5b62101348] [Fri Aug  4 06:17:48 2023] [x3108c0s1b1n0] - Abort(1009851023) (rank 48 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x1549b4000000, scnts=0x205b200, sdispls=0x205c1e0, dtype=0x4c001041, rbuf=0x1549b4240000, rcnts=0x205c400, rdispls=0x205c510, datatype=dtype=0x4c001041, comm=comm=0xc4000009) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x1549b4000000, scnts=0x205b200, sdispls=0x205c1e0, dtype=0x4c001041, rbuf=0x1549b4240000, rcnts=0x205c400, rdispls=0x205c510, datatype=dtype=0x4c001041, comm=comm=0xc4000009) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 49] [job id 7a21707f-a375-4c07-bd14-5a5b62101348] [Fri Aug  4 06:17:48 2023] [x3108c0s1b1n0] - Abort(271653519) (rank 49 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14e12e000000, scnts=0x1b18bd0, sdispls=0x1b15a70, dtype=0x4c001041, rbuf=0x14e12e200000, rcnts=0x1b15c90, rdispls=0x1b15da0, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14e12e000000, scnts=0x1b18bd0, sdispls=0x1b15a70, dtype=0x4c001041, rbuf=0x14e12e200000, rcnts=0x1b15c90, rdispls=0x1b15da0, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 41] [job id 7a21707f-a375-4c07-bd14-5a5b62101348] [Fri Aug  4 06:17:48 2023] [x3108c0s19b1n0] - Abort(137435791) (rank 41 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x1509ea000000, scnts=0x1c99bd0, sdispls=0x1c96a70, dtype=0x4c001041, rbuf=0x1509ea200000, rcnts=0x1c96c90, rdispls=0x1c96da0, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x1509ea000000, scnts=0x1c99bd0, sdispls=0x1c96a70, dtype=0x4c001041, rbuf=0x1509ea200000, rcnts=0x1c96c90, rdispls=0x1c96da0, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 55] [job id 7a21707f-a375-4c07-bd14-5a5b62101348] [Fri Aug  4 06:17:48 2023] [x3108c0s25b0n0] - Abort(540088975) (rank 55 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x147a32000000, scnts=0x2276bd0, sdispls=0x2273a70, dtype=0x4c001041, rbuf=0x147a32200000, rcnts=0x2273c90, rdispls=0x2273da0, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x147a32000000, scnts=0x2276bd0, sdispls=0x2273a70, dtype=0x4c001041, rbuf=0x147a32200000, rcnts=0x2273c90, rdispls=0x2273da0, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 60] [job id 7a21707f-a375-4c07-bd14-5a5b62101348] [Fri Aug  4 06:17:48 2023] [x3108c0s31b0n0] - Abort(204544655) (rank 60 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14d1de000000, scnts=0x2717280, sdispls=0x2718270, dtype=0x4c001041, rbuf=0x14d1de200000, rcnts=0x2718490, rdispls=0x27185a0, datatype=dtype=0x4c001041, comm=comm=0xc4000009) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14d1de000000, scnts=0x2717280, sdispls=0x2718270, dtype=0x4c001041, rbuf=0x14d1de200000, rcnts=0x2718490, rdispls=0x27185a0, datatype=dtype=0x4c001041, comm=comm=0xc4000009) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 10] [job id 7a21707f-a375-4c07-bd14-5a5b62101348] [Fri Aug  4 06:17:48 2023] [x3107c0s31b1n0] - Abort(204544655) (rank 10 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14b984000000, scnts=0x24e20a0, sdispls=0x24e2f40, dtype=0x4c001041, rbuf=0x14b984200000, rcnts=0x24e3160, rdispls=0x24e3270, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14b984000000, scnts=0x24e20a0, sdispls=0x24e2f40, dtype=0x4c001041, rbuf=0x14b984200000, rcnts=0x24e3160, rdispls=0x24e3270, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 22] [job id 7a21707f-a375-4c07-bd14-5a5b62101348] [Fri Aug  4 06:17:48 2023] [x3107c0s7b0n0] - Abort(70326927) (rank 22 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14f76a000000, scnts=0x130b0a0, sdispls=0x130bf40, dtype=0x4c001041, rbuf=0x14f76a200000, rcnts=0x130c160, rdispls=0x130c270, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14f76a000000, scnts=0x130b0a0, sdispls=0x130bf40, dtype=0x4c001041, rbuf=0x14f76a200000, rcnts=0x130c160, rdispls=0x130c270, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)
MPICH ERROR [Rank 32] [job id 7a21707f-a375-4c07-bd14-5a5b62101348] [Fri Aug  4 06:17:48 2023] [x3108c0s13b1n0] - Abort(137435791) (rank 32 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x145f9c000000, scnts=0x1076010, sdispls=0x1077000, dtype=0x4c001041, rbuf=0x145f9c240000, rcnts=0x1077220, rdispls=0x1077330, datatype=dtype=0x4c001041, comm=comm=0xc4000009) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x145f9c000000, scnts=0x1076010, sdispls=0x1077000, dtype=0x4c001041, rbuf=0x145f9c240000, rcnts=0x1077220, rdispls=0x1077330, datatype=dtype=0x4c001041, comm=comm=0xc4000009) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 1] [job id 7a21707f-a375-4c07-bd14-5a5b62101348] [Fri Aug  4 06:17:48 2023] [x3107c0s25b1n0] - Abort(70326927) (rank 1 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14917a000000, scnts=0xf69e50, sdispls=0xf6aba0, dtype=0x4c001041, rbuf=0x14917a200000, rcnts=0xf6adc0, rdispls=0xf6aed0, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14917a000000, scnts=0xf69e50, sdispls=0xf6aba0, dtype=0x4c001041, rbuf=0x14917a200000, rcnts=0xf6adc0, rdispls=0xf6aed0, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 40] [job id 7a21707f-a375-4c07-bd14-5a5b62101348] [Fri Aug  4 06:17:48 2023] [x3108c0s19b1n0] - Abort(1009851023) (rank 40 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x1519c2000000, scnts=0xe31750, sdispls=0xe32740, dtype=0x4c001041, rbuf=0x1519c2200000, rcnts=0xe32960, rdispls=0xe32a70, datatype=dtype=0x4c001041, comm=comm=0xc4000009) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x1519c2000000, scnts=0xe31750, sdispls=0xe32740, dtype=0x4c001041, rbuf=0x1519c2200000, rcnts=0xe32960, rdispls=0xe32a70, datatype=dtype=0x4c001041, comm=comm=0xc4000009) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 9] [job id 7a21707f-a375-4c07-bd14-5a5b62101348] [Fri Aug  4 06:17:48 2023] [x3107c0s31b1n0] - Abort(741415567) (rank 9 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14d42a000000, scnts=0x105bbd0, sdispls=0x1058a70, dtype=0x4c001041, rbuf=0x14d42a200000, rcnts=0x1058c90, rdispls=0x1058da0, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14d42a000000, scnts=0x105bbd0, sdispls=0x1058a70, dtype=0x4c001041, rbuf=0x14d42a200000, rcnts=0x1058c90, rdispls=0x1058da0, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 12] [job id 7a21707f-a375-4c07-bd14-5a5b62101348] [Fri Aug  4 06:17:48 2023] [x3107c0s37b0n0] - Abort(1009851023) (rank 12 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14dd3a000000, scnts=0x2289fe0, sdispls=0x228afd0, dtype=0x4c001041, rbuf=0x14dd3a200000, rcnts=0x228b1f0, rdispls=0x228b300, datatype=dtype=0x4c001041, comm=comm=0xc4000009) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14dd3a000000, scnts=0x2289fe0, sdispls=0x228afd0, dtype=0x4c001041, rbuf=0x14dd3a200000, rcnts=0x228b1f0, rdispls=0x228b300, datatype=dtype=0x4c001041, comm=comm=0xc4000009) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 46] [job id 7a21707f-a375-4c07-bd14-5a5b62101348] [Fri Aug  4 06:17:48 2023] [x3108c0s1b0n0] - Abort(472980111) (rank 46 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14e3fa000000, scnts=0x15220a0, sdispls=0x1522f40, dtype=0x4c001041, rbuf=0x14e3fa200000, rcnts=0x1523160, rdispls=0x1523270, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14e3fa000000, scnts=0x15220a0, sdispls=0x1522f40, dtype=0x4c001041, rbuf=0x14e3fa200000, rcnts=0x1523160, rdispls=0x1523270, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 34] [job id 7a21707f-a375-4c07-bd14-5a5b62101348] [Fri Aug  4 06:17:48 2023] [x3108c0s13b1n0] - Abort(607197839) (rank 34 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x148394000000, scnts=0x1c330a0, sdispls=0x1c33f40, dtype=0x4c001041, rbuf=0x148394200000, rcnts=0x1c34160, rdispls=0x1c34270, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x148394000000, scnts=0x1c330a0, sdispls=0x1c33f40, dtype=0x4c001041, rbuf=0x148394200000, rcnts=0x1c34160, rdispls=0x1c34270, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 15] [job id 7a21707f-a375-4c07-bd14-5a5b62101348] [Fri Aug  4 06:17:48 2023] [x3107c0s37b0n0] - Abort(808524431) (rank 15 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x145ae2000000, scnts=0x14d5bd0, sdispls=0x14d2a70, dtype=0x4c001041, rbuf=0x145ae2200000, rcnts=0x14d2c90, rdispls=0x14d2da0, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x145ae2000000, scnts=0x14d5bd0, sdispls=0x14d2a70, dtype=0x4c001041, rbuf=0x145ae2200000, rcnts=0x14d2c90, rdispls=0x14d2da0, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 27] [job id 7a21707f-a375-4c07-bd14-5a5b62101348] [Fri Aug  4 06:17:48 2023] [x3107c0s7b1n0] - Abort(204544655) (rank 27 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14fee4000000, scnts=0x1fa1bd0, sdispls=0x1f9ea70, dtype=0x4c001041, rbuf=0x14fee4200000, rcnts=0x1f9ec90, rdispls=0x1f9eda0, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14fee4000000, scnts=0x1fa1bd0, sdispls=0x1f9ea70, dtype=0x4c001041, rbuf=0x14fee4200000, rcnts=0x1f9ec90, rdispls=0x1f9eda0, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 45] [job id 7a21707f-a375-4c07-bd14-5a5b62101348] [Fri Aug  4 06:17:48 2023] [x3108c0s1b0n0] - Abort(540088975) (rank 45 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x15206e000000, scnts=0x1492bd0, sdispls=0x148fa70, dtype=0x4c001041, rbuf=0x15206e200000, rcnts=0x148fc90, rdispls=0x148fda0, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x15206e000000, scnts=0x1492bd0, sdispls=0x148fa70, dtype=0x4c001041, rbuf=0x15206e200000, rcnts=0x148fc90, rdispls=0x148fda0, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 35] [job id 7a21707f-a375-4c07-bd14-5a5b62101348] [Fri Aug  4 06:17:48 2023] [x3108c0s13b1n0] - Abort(405871247) (rank 35 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x149fba000000, scnts=0x1e93bd0, sdispls=0x1e90a70, dtype=0x4c001041, rbuf=0x149fba200000, rcnts=0x1e90c90, rdispls=0x1e90da0, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x149fba000000, scnts=0x1e93bd0, sdispls=0x1e90a70, dtype=0x4c001041, rbuf=0x149fba200000, rcnts=0x1e90c90, rdispls=0x1e90da0, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 28] [job id 7a21707f-a375-4c07-bd14-5a5b62101348] [Fri Aug  4 06:17:48 2023] [x3108c0s13b0n0] - Abort(3218063) (rank 28 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x151bae000000, scnts=0x16714c0, sdispls=0x16724b0, dtype=0x4c001041, rbuf=0x151bae200000, rcnts=0x16726d0, rdispls=0x16727e0, datatype=dtype=0x4c001041, comm=comm=0xc4000009) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x151bae000000, scnts=0x16714c0, sdispls=0x16724b0, dtype=0x4c001041, rbuf=0x151bae200000, rcnts=0x16726d0, rdispls=0x16727e0, datatype=dtype=0x4c001041, comm=comm=0xc4000009) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 24] [job id 7a21707f-a375-4c07-bd14-5a5b62101348] [Fri Aug  4 06:17:48 2023] [x3107c0s7b1n0] - Abort(3218063) (rank 24 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x152e9e000000, scnts=0x20e6560, sdispls=0x20e7550, dtype=0x4c001041, rbuf=0x152e9e200000, rcnts=0x20e7770, rdispls=0x20e7880, datatype=dtype=0x4c001041, comm=comm=0xc4000009) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x152e9e000000, scnts=0x20e6560, sdispls=0x20e7550, dtype=0x4c001041, rbuf=0x152e9e200000, rcnts=0x20e7770, rdispls=0x20e7880, datatype=dtype=0x4c001041, comm=comm=0xc4000009) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)
MPICH ERROR [Rank 54] [job id 7a21707f-a375-4c07-bd14-5a5b62101348] [Fri Aug  4 06:17:48 2023] [x3108c0s25b0n0] - Abort(875633295) (rank 54 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x149aea000000, scnts=0x169c0a0, sdispls=0x169cf40, dtype=0x4c001041, rbuf=0x149aea200000, rcnts=0x169d160, rdispls=0x169d270, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x149aea000000, scnts=0x169c0a0, sdispls=0x169cf40, dtype=0x4c001041, rbuf=0x149aea200000, rcnts=0x169d160, rdispls=0x169d270, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 58] [job id 7a21707f-a375-4c07-bd14-5a5b62101348] [Fri Aug  4 06:17:48 2023] [x3108c0s25b1n0] - Abort(741415567) (rank 58 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x145cfa000000, scnts=0x260c0a0, sdispls=0x260cf40, dtype=0x4c001041, rbuf=0x145cfa200000, rcnts=0x260d160, rdispls=0x260d270, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x145cfa000000, scnts=0x260c0a0, sdispls=0x260cf40, dtype=0x4c001041, rbuf=0x145cfa200000, rcnts=0x260d160, rdispls=0x260d270, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
x3108c0s13b0n0.hsn.cm.polaris.alcf.anl.gov: rank 31 exited with code 255
