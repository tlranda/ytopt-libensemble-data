“RANK= 21 LOCAL_RANK= 1 gpu= 2”
“RANK= 23 LOCAL_RANK= 3 gpu= 0”
“RANK= 20 LOCAL_RANK= 0 gpu= 3”
“RANK= 16 LOCAL_RANK= 0 gpu= 3”
“RANK= 17 LOCAL_RANK= 1 gpu= 2”
“RANK= 19 LOCAL_RANK= 3 gpu= 0”
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -n x ']'
+ set +x
“RANK= 22 LOCAL_RANK= 2 gpu= 1”
Shell debugging restarted
“RANK= 18 LOCAL_RANK= 2 gpu= 1”
+ unset __lmod_vx
+ speed3d_r2c cufft double 128 128 128 -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 16 2 1
+ '[' -z '' ']'
+ case "$-" in
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
Shell debugging restarted
+ unset __lmod_vx
+ unset __lmod_vx
+ speed3d_r2c cufft double 128 128 128 -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 16 2 1
+ speed3d_r2c cufft double 128 128 128 -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 16 2 1
+ '[' -z '' ']'
+ '[' -z '' ']'
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 128 128 128 -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 16 2 1
+ unset __lmod_vx
+ speed3d_r2c cufft double 128 128 128 -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 16 2 1
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
Shell debugging restarted
+ unset __lmod_vx
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ speed3d_r2c cufft double 128 128 128 -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 16 2 1
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
Shell debugging restarted
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ unset __lmod_vx
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ speed3d_r2c cufft double 128 128 128 -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 16 2 1
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 128 128 128 -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 16 2 1
“RANK= 4 LOCAL_RANK= 0 gpu= 3”
“RANK= 5 LOCAL_RANK= 1 gpu= 2”
“RANK= 12 LOCAL_RANK= 0 gpu= 3”
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 128 128 128 -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 16 2 1
“RANK= 14 LOCAL_RANK= 2 gpu= 1”
“RANK= 13 LOCAL_RANK= 1 gpu= 2”
“RANK= 15 LOCAL_RANK= 3 gpu= 0”
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 128 128 128 -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 16 2 1
“RANK= 7 LOCAL_RANK= 3 gpu= 0”
+ '[' -z '' ']'
+ case "$-" in
+ '[' -z '' ']'
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
Shell debugging restarted
+ unset __lmod_vx
+ unset __lmod_vx
“RANK= 6 LOCAL_RANK= 2 gpu= 1”
+ speed3d_r2c cufft double 128 128 128 -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 16 2 1
+ speed3d_r2c cufft double 128 128 128 -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 16 2 1
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ unset __lmod_vx
+ speed3d_r2c cufft double 128 128 128 -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 16 2 1
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 128 128 128 -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 16 2 1
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 128 128 128 -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 16 2 1
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 128 128 128 -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 16 2 1
“RANK= 26 LOCAL_RANK= 2 gpu= 1”
“RANK= 24 LOCAL_RANK= 0 gpu= 3”
“RANK= 27 LOCAL_RANK= 3 gpu= 0”
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
“RANK= 25 LOCAL_RANK= 1 gpu= 2”
+ speed3d_r2c cufft double 128 128 128 -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 16 2 1
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 128 128 128 -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 16 2 1
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 128 128 128 -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 16 2 1
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 128 128 128 -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 16 2 1
“RANK= 29 LOCAL_RANK= 1 gpu= 2”
“RANK= 31 LOCAL_RANK= 3 gpu= 0”
“RANK= 30 LOCAL_RANK= 2 gpu= 1”
“RANK= 10 LOCAL_RANK= 2 gpu= 1”
“RANK= 28 LOCAL_RANK= 0 gpu= 3”
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
Shell debugging restarted
+ unset __lmod_vx
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 128 128 128 -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 16 2 1
+ speed3d_r2c cufft double 128 128 128 -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 16 2 1
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 128 128 128 -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 16 2 1
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 128 128 128 -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 16 2 1
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 128 128 128 -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 16 2 1
“RANK= 9 LOCAL_RANK= 1 gpu= 2”
“RANK= 11 LOCAL_RANK= 3 gpu= 0”
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 128 128 128 -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 16 2 1
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 128 128 128 -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 16 2 1
“RANK= 8 LOCAL_RANK= 0 gpu= 3”
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 128 128 128 -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 16 2 1
“RANK= 1 LOCAL_RANK= 1 gpu= 2”
“RANK= 2 LOCAL_RANK= 2 gpu= 1”
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 128 128 128 -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 16 2 1
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 128 128 128 -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 16 2 1
“RANK= 3 LOCAL_RANK= 3 gpu= 0”
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 128 128 128 -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 16 2 1
“RANK= 0 LOCAL_RANK= 0 gpu= 3”
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 128 128 128 -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 16 2 1
MPICH ERROR [Rank 19] [job id d1708417-af55-4d0b-ac4b-ac67a68f5e69] [Thu Jun 29 02:46:47 2023] [x3111c0s19b0n0] - Abort(204544655) (rank 19 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14d8d3800000, scnts=0x2459860, sdispls=0x2496ca0, dtype=0x4c001041, rbuf=0x14d8d3880000, rcnts=0x245a1a0, rdispls=0x245a230, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14d8d3800000, scnts=0x2459860, sdispls=0x2496ca0, dtype=0x4c001041, rbuf=0x14d8d3880000, rcnts=0x245a1a0, rdispls=0x245a230, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 25] [job id d1708417-af55-4d0b-ac4b-ac67a68f5e69] [Thu Jun 29 02:46:47 2023] [x3111c0s1b0n0] - Abort(271653519) (rank 25 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14da53800000, scnts=0x24ba6f0, sdispls=0x24f7c20, dtype=0x4c001041, rbuf=0x14da53880000, rcnts=0x24bb030, rdispls=0x24bb0c0, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14da53800000, scnts=0x24ba6f0, sdispls=0x24f7c20, dtype=0x4c001041, rbuf=0x14da53880000, rcnts=0x24bb030, rdispls=0x24bb0c0, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 31] [job id d1708417-af55-4d0b-ac4b-ac67a68f5e69] [Thu Jun 29 02:46:47 2023] [x3111c0s1b1n0] - Abort(472980111) (rank 31 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14f9f3800000, scnts=0x161f660, sdispls=0x165cb80, dtype=0x4c001041, rbuf=0x14f9f3880000, rcnts=0x161ffa0, rdispls=0x1620030, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14f9f3800000, scnts=0x161f660, sdispls=0x165cb80, dtype=0x4c001041, rbuf=0x14f9f3880000, rcnts=0x161ffa0, rdispls=0x1620030, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 22] [job id d1708417-af55-4d0b-ac4b-ac67a68f5e69] [Thu Jun 29 02:46:47 2023] [x3111c0s19b1n0] - Abort(741415567) (rank 22 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x154883800000, scnts=0x14a53a0, sdispls=0x14a5c50, dtype=0x4c001041, rbuf=0x154883880000, rcnts=0x14a5d70, rdispls=0x14a5e00, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x154883800000, scnts=0x14a53a0, sdispls=0x14a5c50, dtype=0x4c001041, rbuf=0x154883880000, rcnts=0x14a5d70, rdispls=0x14a5e00, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 28] [job id d1708417-af55-4d0b-ac4b-ac67a68f5e69] [Thu Jun 29 02:46:47 2023] [x3111c0s1b1n0] - Abort(472980111) (rank 28 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14c203800000, scnts=0x2786e10, sdispls=0x2787950, dtype=0x4c001041, rbuf=0x14c203880000, rcnts=0x2787a70, rdispls=0x2787b00, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14c203800000, scnts=0x2786e10, sdispls=0x2787950, dtype=0x4c001041, rbuf=0x14c203880000, rcnts=0x2787a70, rdispls=0x2787b00, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 21] [job id d1708417-af55-4d0b-ac4b-ac67a68f5e69] [Thu Jun 29 02:46:47 2023] [x3111c0s19b1n0] - Abort(875633295) (rank 21 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x152fc3800000, scnts=0x20df3a0, sdispls=0x20dfc50, dtype=0x4c001041, rbuf=0x152fc3880000, rcnts=0x20dfd70, rdispls=0x20dfe00, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x152fc3800000, scnts=0x20df3a0, sdispls=0x20dfc50, dtype=0x4c001041, rbuf=0x152fc3880000, rcnts=0x20dfd70, rdispls=0x20dfe00, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 18] [job id d1708417-af55-4d0b-ac4b-ac67a68f5e69] [Thu Jun 29 02:46:47 2023] [x3111c0s19b0n0] - Abort(942742159) (rank 18 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x145b2b800000, scnts=0x2160860, sdispls=0x219dc30, dtype=0x4c001041, rbuf=0x145b2b880000, rcnts=0x2161110, rdispls=0x21611a0, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x145b2b800000, scnts=0x2160860, sdispls=0x219dc30, dtype=0x4c001041, rbuf=0x145b2b880000, rcnts=0x2161110, rdispls=0x21611a0, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 30] [job id d1708417-af55-4d0b-ac4b-ac67a68f5e69] [Thu Jun 29 02:46:47 2023] [x3111c0s1b1n0] - Abort(3218063) (rank 30 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x154573800000, scnts=0x11cc660, sdispls=0x1209b80, dtype=0x4c001041, rbuf=0x154573880000, rcnts=0x11ccfa0, rdispls=0x11cd030, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x154573800000, scnts=0x11cc660, sdispls=0x1209b80, dtype=0x4c001041, rbuf=0x154573880000, rcnts=0x11ccfa0, rdispls=0x11cd030, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 24] [job id d1708417-af55-4d0b-ac4b-ac67a68f5e69] [Thu Jun 29 02:46:47 2023] [x3111c0s1b0n0] - Abort(674306703) (rank 24 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x15529b800000, scnts=0x1c62350, sdispls=0x1c62e90, dtype=0x4c001041, rbuf=0x15529b880000, rcnts=0x1c64f20, rdispls=0x1c7f1d0, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x15529b800000, scnts=0x1c62350, sdispls=0x1c62e90, dtype=0x4c001041, rbuf=0x15529b880000, rcnts=0x1c64f20, rdispls=0x1c7f1d0, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 16] [job id d1708417-af55-4d0b-ac4b-ac67a68f5e69] [Thu Jun 29 02:46:47 2023] [x3111c0s19b0n0] - Abort(606673423) (rank 16 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389).........: MPI_Alltoallv(sbuf=0x14b933800000, scnts=0xb5f3d0, sdispls=0xb5ff10, dtype=0x4c001041, rbuf=0x14b933880000, rcnts=0xb7c1d0, rdispls=0xb7c260, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1155)...: 
MPIC_Isend(511).............: 
MPID_Isend_coll(610)........: 
MPIDI_isend_coll_unsafe(176): 
MPIDI_OFI_send_normal(352)..: OFI tagged senddata failed (ofi_send.h:352:MPIDI_OFI_send_normal:No route to host)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389).........: MPI_Alltoallv(sbuf=0x14b933800000, scnts=0xb5f3d0, sdispls=0xb5ff10, dtype=0x4c001041, rbuf=0x14b933880000, rcnts=0xb7c1d0, rdispls=0xb7c260, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1155)...: 
MPIC_Isend(511).............: 
MPID_Isend_coll(610)........: 
MPIDI_isend_coll_unsafe(176): 
MPIDI_OFI_send_normal(352)..: OFI tagged senddata failed (ofi_send.h:352:MPIDI_OFI_send_normal:No route to host)
MPICH ERROR [Rank 29] [job id d1708417-af55-4d0b-ac4b-ac67a68f5e69] [Thu Jun 29 02:46:47 2023] [x3111c0s1b1n0] - Abort(2693647) (rank 29 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389).........: MPI_Alltoallv(sbuf=0x153c63800000, scnts=0xd86ab0, sdispls=0xdc3300, dtype=0x4c001041, rbuf=0x153c63880000, rcnts=0xd873f0, rdispls=0xd87480, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1155)...: 
MPIC_Isend(511).............: 
MPID_Isend_coll(610)........: 
MPIDI_isend_coll_unsafe(176): 
MPIDI_OFI_send_normal(352)..: OFI tagged senddata failed (ofi_send.h:352:MPIDI_OFI_send_normal:No route to host)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389).........: MPI_Alltoallv(sbuf=0x153c63800000, scnts=0xd86ab0, sdispls=0xdc3300, dtype=0x4c001041, rbuf=0x153c63880000, rcnts=0xd873f0, rdispls=0xd87480, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1155)...: 
MPIC_Isend(511).............: 
MPID_Isend_coll(610)........: 
MPIDI_isend_coll_unsafe(176): 
MPIDI_OFI_send_normal(352)..: OFI tagged senddata failed (ofi_send.h:352:MPIDI_OFI_send_normal:No route to host)
x3111c0s19b0n0.hsn.cm.polaris.alcf.anl.gov: rank 19 exited with code 255
MPICH ERROR [Rank 23] [job id d1708417-af55-4d0b-ac4b-ac67a68f5e69] [Thu Jun 29 02:46:47 2023] [x3111c0s19b1n0] - Abort(472455695) (rank 23 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389).........: MPI_Alltoallv(sbuf=0x14c65b800000, scnts=0x103b3a0, sdispls=0x103bc50, dtype=0x4c001041, rbuf=0x14c65b880000, rcnts=0x103bd70, rdispls=0x103be00, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1155)...: 
MPIC_Isend(511).............: 
MPID_Isend_coll(610)........: 
MPIDI_isend_coll_unsafe(176): 
MPIDI_OFI_send_normal(352)..: OFI tagged senddata failed (ofi_send.h:352:MPIDI_OFI_send_normal:No route to host)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389).........: MPI_Alltoallv(sbuf=0x14c65b800000, scnts=0x103b3a0, sdispls=0x103bc50, dtype=0x4c001041, rbuf=0x14c65b880000, rcnts=0x103bd70, rdispls=0x103be00, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1155)...: 
MPIC_Isend(511).............: 
MPID_Isend_coll(610)........: 
MPIDI_isend_coll_unsafe(176): 
MPIDI_OFI_send_normal(352)..: OFI tagged senddata failed (ofi_send.h:352:MPIDI_OFI_send_normal:No route to host)
MPICH ERROR [Rank 8] [job id d1708417-af55-4d0b-ac4b-ac67a68f5e69] [Thu Jun 29 02:46:47 2023] [x3111c0s13b0n0] - Abort(1009851023) (rank 8 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14d123800000, scnts=0x1a81890, sdispls=0x1a9e6b0, dtype=0x4c001041, rbuf=0x14d123884000, rcnts=0x1a9e7d0, rdispls=0x1a9e860, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14d123800000, scnts=0x1a81890, sdispls=0x1a9e6b0, dtype=0x4c001041, rbuf=0x14d123884000, rcnts=0x1a9e7d0, rdispls=0x1a9e860, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 20] [job id d1708417-af55-4d0b-ac4b-ac67a68f5e69] [Thu Jun 29 02:46:47 2023] [x3111c0s19b1n0] - Abort(1009851023) (rank 20 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14c003800000, scnts=0x26accb0, sdispls=0x26ad7f0, dtype=0x4c001041, rbuf=0x14c003880000, rcnts=0x26ad910, rdispls=0x26ad9a0, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14c003800000, scnts=0x26accb0, sdispls=0x26ad7f0, dtype=0x4c001041, rbuf=0x14c003880000, rcnts=0x26ad910, rdispls=0x26ad9a0, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)
MPICH ERROR [Rank 26] [job id d1708417-af55-4d0b-ac4b-ac67a68f5e69] [Thu Jun 29 02:46:47 2023] [x3111c0s1b0n0] - Abort(740891151) (rank 26 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389).........: MPI_Alltoallv(sbuf=0x1520b3800000, scnts=0x13f1ff0, sdispls=0x13f1630, dtype=0x4c001041, rbuf=0x1520b3880000, rcnts=0x13f28a0, rdispls=0x13f2930, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1155)...: 
MPIC_Isend(511).............: 
MPID_Isend_coll(610)........: 
MPIDI_isend_coll_unsafe(176): 
MPIDI_OFI_send_normal(352)..: OFI tagged senddata failed (ofi_send.h:352:MPIDI_OFI_send_normal:No route to host)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389).........: MPI_Alltoallv(sbuf=0x1520b3800000, scnts=0x13f1ff0, sdispls=0x13f1630, dtype=0x4c001041, rbuf=0x1520b3880000, rcnts=0x13f28a0, rdispls=0x13f2930, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1155)...: 
MPIC_Isend(511).............: 
MPID_Isend_coll(610)........: 
MPIDI_isend_coll_unsafe(176): 
MPIDI_OFI_send_normal(352)..: OFI tagged senddata failed (ofi_send.h:352:MPIDI_OFI_send_normal:No route to host)
MPICH ERROR [Rank 17] [job id d1708417-af55-4d0b-ac4b-ac67a68f5e69] [Thu Jun 29 02:46:47 2023] [x3111c0s19b0n0] - Abort(472980111) (rank 17 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x1528a3800000, scnts=0x1205840, sdispls=0x1242c80, dtype=0x4c001041, rbuf=0x1528a3880000, rcnts=0x1206180, rdispls=0x1206210, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x1528a3800000, scnts=0x1205840, sdispls=0x1242c80, dtype=0x4c001041, rbuf=0x1528a3880000, rcnts=0x1206180, rdispls=0x1206210, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)
MPICH ERROR [Rank 3] [job id d1708417-af55-4d0b-ac4b-ac67a68f5e69] [Thu Jun 29 02:46:47 2023] [x3110c0s7b0n0] - Abort(472980111) (rank 3 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x15008b800000, scnts=0x245ea40, sdispls=0x245f2f0, dtype=0x4c001041, rbuf=0x15008b884000, rcnts=0x245f410, rdispls=0x245f4a0, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x15008b800000, scnts=0x245ea40, sdispls=0x245f2f0, dtype=0x4c001041, rbuf=0x15008b884000, rcnts=0x245f410, rdispls=0x245f4a0, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 1] [job id d1708417-af55-4d0b-ac4b-ac67a68f5e69] [Thu Jun 29 02:46:47 2023] [x3110c0s7b0n0] - Abort(808000015) (rank 1 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389).........: MPI_Alltoallv(sbuf=0x149ae3800000, scnts=0x2002da0, sdispls=0x2002450, dtype=0x4c001041, rbuf=0x149ae3884000, rcnts=0x1fc58e0, rdispls=0x1fc5970, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1155)...: 
MPIC_Isend(511).............: 
MPID_Isend_coll(610)........: 
MPIDI_isend_coll_unsafe(176): 
MPIDI_OFI_send_normal(352)..: OFI tagged senddata failed (ofi_send.h:352:MPIDI_OFI_send_normal:No route to host)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389).........: MPI_Alltoallv(sbuf=0x149ae3800000, scnts=0x2002da0, sdispls=0x2002450, dtype=0x4c001041, rbuf=0x149ae3884000, rcnts=0x1fc58e0, rdispls=0x1fc5970, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1155)...: 
MPIC_Isend(511).............: 
MPID_Isend_coll(610)........: 
MPIDI_isend_coll_unsafe(176): 
MPIDI_OFI_send_normal(352)..: OFI tagged senddata failed (ofi_send.h:352:MPIDI_OFI_send_normal:No route to host)
MPICH ERROR [Rank 2] [job id d1708417-af55-4d0b-ac4b-ac67a68f5e69] [Thu Jun 29 02:46:47 2023] [x3110c0s7b0n0] - Abort(405871247) (rank 2 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14ec8b800000, scnts=0x157c180, sdispls=0x157ca30, dtype=0x4c001041, rbuf=0x14ec8b884000, rcnts=0x157cb50, rdispls=0x157cbe0, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14ec8b800000, scnts=0x157c180, sdispls=0x157ca30, dtype=0x4c001041, rbuf=0x14ec8b884000, rcnts=0x157cb50, rdispls=0x157cbe0, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 6] [job id d1708417-af55-4d0b-ac4b-ac67a68f5e69] [Thu Jun 29 02:46:47 2023] [x3110c0s7b1n0] - Abort(1009851023) (rank 6 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14e0ab800000, scnts=0x1700210, sdispls=0x1700ac0, dtype=0x4c001041, rbuf=0x14e0ab884000, rcnts=0x1700be0, rdispls=0x1700c70, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14e0ab800000, scnts=0x1700210, sdispls=0x1700ac0, dtype=0x4c001041, rbuf=0x14e0ab884000, rcnts=0x1700be0, rdispls=0x1700c70, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 0] [job id d1708417-af55-4d0b-ac4b-ac67a68f5e69] [Thu Jun 29 02:46:47 2023] [x3110c0s7b0n0] - Abort(204544655) (rank 0 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14bdf3800000, scnts=0x148c4a0, sdispls=0x14a9250, dtype=0x4c001041, rbuf=0x14bdf3884000, rcnts=0x14a9370, rdispls=0x14a9400, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14bdf3800000, scnts=0x148c4a0, sdispls=0x14a9250, dtype=0x4c001041, rbuf=0x14bdf3884000, rcnts=0x14a9370, rdispls=0x14a9400, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
Signal forwarding failed: Application d1708417-af55-4d0b-ac4b-ac67a68f5e69 not found
