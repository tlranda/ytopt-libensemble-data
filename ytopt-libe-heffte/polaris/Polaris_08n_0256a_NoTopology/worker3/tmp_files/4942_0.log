“RANK= 8 LOCAL_RANK= 0 gpu= 3”
“RANK= 9 LOCAL_RANK= 1 gpu= 2”
“RANK= 11 LOCAL_RANK= 3 gpu= 0”
“RANK= 10 LOCAL_RANK= 2 gpu= 1”
“RANK= 4 LOCAL_RANK= 0 gpu= 3”
“RANK= 6 LOCAL_RANK= 2 gpu= 1”
“RANK= 7 LOCAL_RANK= 3 gpu= 0”
“RANK= 5 LOCAL_RANK= 1 gpu= 2”
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
“RANK= 2 LOCAL_RANK= 2 gpu= 1”
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -n x ']'
+ set +x
Shell debugging restarted
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 32 1 1 -n5
“RANK= 0 LOCAL_RANK= 0 gpu= 3”
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 32 1 1 -n5
“RANK= 3 LOCAL_RANK= 3 gpu= 0”
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 32 1 1 -n5
Shell debugging restarted
+ unset __lmod_vx
“RANK= 1 LOCAL_RANK= 1 gpu= 2”
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 32 1 1 -n5
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
“RANK= 23 LOCAL_RANK= 3 gpu= 0”
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
“RANK= 22 LOCAL_RANK= 2 gpu= 1”
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
“RANK= 20 LOCAL_RANK= 0 gpu= 3”
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
“RANK= 21 LOCAL_RANK= 1 gpu= 2”
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 32 1 1 -n5
Shell debugging restarted
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 32 1 1 -n5
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 32 1 1 -n5
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 32 1 1 -n5
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -n x ']'
+ set +x
“RANK= 12 LOCAL_RANK= 0 gpu= 3”
“RANK= 13 LOCAL_RANK= 1 gpu= 2”
“RANK= 15 LOCAL_RANK= 3 gpu= 0”
“RANK= 14 LOCAL_RANK= 2 gpu= 1”
Shell debugging restarted
+ unset __lmod_vx
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 32 1 1 -n5
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
“RANK= 24 LOCAL_RANK= 0 gpu= 3”
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 32 1 1 -n5
“RANK= 27 LOCAL_RANK= 3 gpu= 0”
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 32 1 1 -n5
“RANK= 26 LOCAL_RANK= 2 gpu= 1”
Shell debugging restarted
“RANK= 25 LOCAL_RANK= 1 gpu= 2”
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 32 1 1 -n5
“RANK= 28 LOCAL_RANK= 0 gpu= 3”
“RANK= 29 LOCAL_RANK= 1 gpu= 2”
“RANK= 30 LOCAL_RANK= 2 gpu= 1”
“RANK= 31 LOCAL_RANK= 3 gpu= 0”
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 32 1 1 -n5
Shell debugging restarted
+ unset __lmod_vx
+ '[' -z '' ']'
Shell debugging restarted
+ unset __lmod_vx
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 32 1 1 -n5
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 32 1 1 -n5
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 32 1 1 -n5
Shell debugging restarted
+ unset __lmod_vx
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
“RANK= 16 LOCAL_RANK= 0 gpu= 3”
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
“RANK= 19 LOCAL_RANK= 3 gpu= 0”
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 32 1 1 -n5
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
“RANK= 17 LOCAL_RANK= 1 gpu= 2”
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 32 1 1 -n5
“RANK= 18 LOCAL_RANK= 2 gpu= 1”
+ '[' -z '' ']'
+ case "$-" in
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 32 1 1 -n5
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 32 1 1 -n5
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 32 1 1 -n5
Shell debugging restarted
Shell debugging restarted
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 32 1 1 -n5
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 32 1 1 -n5
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 32 1 1 -n5
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 32 1 1 -n5
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 32 1 1 -n5
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 32 1 1 -n5
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 32 1 1 -n5
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ '[' -z '' ']'
+ '[' -z '' ']'
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 32 1 1 -n5
Shell debugging restarted
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 32 1 1 -n5
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 32 1 1 -n5
+ speed3d_r2c cufft float 256 256 256 -no-reorder -a2a -slabs -r2c_dir 1 -ingrid 32 1 1 -outgrid 32 1 1 -n5
MPICH ERROR [Rank 16] [job id 74849796-a6c7-4894-8ff5-a4fd513bcd64] [Fri Nov  3 04:27:47 2023] [x3001c0s1b1n0] - Abort(338762127) (rank 16 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x14b03f800000, scount=8448, dtype=0x4c000840, rbuf=0x14b03fa10000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x14b03f800000, scount=8448, dtype=0x4c000840, rbuf=0x14b03fa10000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)
MPICH ERROR [Rank 11] [job id 74849796-a6c7-4894-8ff5-a4fd513bcd64] [Fri Nov  3 04:27:47 2023] [x3001c0s19b1n0] - Abort(540088719) (rank 11 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x14dc63800000, scount=8448, dtype=0x4c000840, rbuf=0x14dc63a10000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x14dc63800000, scount=8448, dtype=0x4c000840, rbuf=0x14dc63a10000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 0] [job id 74849796-a6c7-4894-8ff5-a4fd513bcd64] [Fri Nov  3 04:27:47 2023] [x3001c0s13b1n0] - Abort(338762127) (rank 0 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x14de37800000, scount=8448, dtype=0x4c000840, rbuf=0x14de37a10000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x14de37800000, scount=8448, dtype=0x4c000840, rbuf=0x14de37a10000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)
MPICH ERROR [Rank 1] [job id 74849796-a6c7-4894-8ff5-a4fd513bcd64] [Fri Nov  3 04:27:47 2023] [x3001c0s13b1n0] - Abort(875633039) (rank 1 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x14a66c600000, scount=8448, dtype=0x4c000840, rbuf=0x14a66c810000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x14a66c600000, scount=8448, dtype=0x4c000840, rbuf=0x14a66c810000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 4] [job id 74849796-a6c7-4894-8ff5-a4fd513bcd64] [Fri Nov  3 04:27:47 2023] [x3001c0s19b0n0] - Abort(674306447) (rank 4 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x14a54a600000, scount=8448, dtype=0x4c000840, rbuf=0x14a54a810000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x14a54a600000, scount=8448, dtype=0x4c000840, rbuf=0x14a54a810000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 28] [job id 74849796-a6c7-4894-8ff5-a4fd513bcd64] [Fri Nov  3 04:27:47 2023] [x3001c0s31b0n0] - Abort(674306447) (rank 28 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x148bc8400000, scount=8448, dtype=0x4c000840, rbuf=0x148bc8610000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x148bc8400000, scount=8448, dtype=0x4c000840, rbuf=0x148bc8610000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 31] [job id 74849796-a6c7-4894-8ff5-a4fd513bcd64] [Fri Nov  3 04:27:47 2023] [x3001c0s31b0n0] - Abort(607197583) (rank 31 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x1504ba400000, scount=8448, dtype=0x4c000840, rbuf=0x1504ba610000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x1504ba400000, scount=8448, dtype=0x4c000840, rbuf=0x1504ba610000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 30] [job id 74849796-a6c7-4894-8ff5-a4fd513bcd64] [Fri Nov  3 04:27:47 2023] [x3001c0s31b0n0] - Abort(472979855) (rank 30 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x1534dc400000, scount=8448, dtype=0x4c000840, rbuf=0x1534dc610000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x1534dc400000, scount=8448, dtype=0x4c000840, rbuf=0x1534dc610000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 5] [job id 74849796-a6c7-4894-8ff5-a4fd513bcd64] [Fri Nov  3 04:27:47 2023] [x3001c0s19b0n0] - Abort(1009850767) (rank 5 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x14a952600000, scount=8448, dtype=0x4c000840, rbuf=0x14a952810000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x14a952600000, scount=8448, dtype=0x4c000840, rbuf=0x14a952810000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 26] [job id 74849796-a6c7-4894-8ff5-a4fd513bcd64] [Fri Nov  3 04:27:47 2023] [x3001c0s25b1n0] - Abort(338762127) (rank 26 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x147f6c400000, scount=8448, dtype=0x4c000840, rbuf=0x147f6c610000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x147f6c400000, scount=8448, dtype=0x4c000840, rbuf=0x147f6c610000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 24] [job id 74849796-a6c7-4894-8ff5-a4fd513bcd64] [Fri Nov  3 04:27:47 2023] [x3001c0s25b1n0] - Abort(137435535) (rank 24 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x15447a400000, scount=8448, dtype=0x4c000840, rbuf=0x15447a610000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x15447a400000, scount=8448, dtype=0x4c000840, rbuf=0x15447a610000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 15] [job id 74849796-a6c7-4894-8ff5-a4fd513bcd64] [Fri Nov  3 04:27:47 2023] [x3001c0s1b0n0] - Abort(1009850767) (rank 15 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x14d06a400000, scount=8448, dtype=0x4c000840, rbuf=0x14d06a610000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x14d06a400000, scount=8448, dtype=0x4c000840, rbuf=0x14d06a610000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 21] [job id 74849796-a6c7-4894-8ff5-a4fd513bcd64] [Fri Nov  3 04:27:47 2023] [x3001c0s25b0n0] - Abort(472979855) (rank 21 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x150f3e400000, scount=8448, dtype=0x4c000840, rbuf=0x150f3e610000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x150f3e400000, scount=8448, dtype=0x4c000840, rbuf=0x150f3e610000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 22] [job id 74849796-a6c7-4894-8ff5-a4fd513bcd64] [Fri Nov  3 04:27:47 2023] [x3001c0s25b0n0] - Abort(1009850767) (rank 22 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x1460da400000, scount=8448, dtype=0x4c000840, rbuf=0x1460da610000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x1460da400000, scount=8448, dtype=0x4c000840, rbuf=0x1460da610000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 13] [job id 74849796-a6c7-4894-8ff5-a4fd513bcd64] [Fri Nov  3 04:27:47 2023] [x3001c0s1b0n0] - Abort(70326671) (rank 13 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x1542c2400000, scount=8448, dtype=0x4c000840, rbuf=0x1542c2610000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x1542c2400000, scount=8448, dtype=0x4c000840, rbuf=0x1542c2610000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 14] [job id 74849796-a6c7-4894-8ff5-a4fd513bcd64] [Fri Nov  3 04:27:47 2023] [x3001c0s1b0n0] - Abort(70326671) (rank 14 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x14f7e2400000, scount=8448, dtype=0x4c000840, rbuf=0x14f7e2610000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

MPICH ERROR [Rank 25] [job id 74849796-a6c7-4894-8ff5-a4fd513bcd64] [Fri Nov  3 04:27:47 2023] [x3001c0s25b1n0] - Abort(472979855) (rank 25 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x153542400000, scount=8448, dtype=0x4c000840, rbuf=0x153542610000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x153542400000, scount=8448, dtype=0x4c000840, rbuf=0x153542610000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x14f7e2400000, scount=8448, dtype=0x4c000840, rbuf=0x14f7e2610000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 20] [job id 74849796-a6c7-4894-8ff5-a4fd513bcd64] [Fri Nov  3 04:27:47 2023] [x3001c0s25b0n0] - Abort(271653263) (rank 20 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x145f6a400000, scount=8448, dtype=0x4c000840, rbuf=0x145f6a610000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x145f6a400000, scount=8448, dtype=0x4c000840, rbuf=0x145f6a610000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 2] [job id 74849796-a6c7-4894-8ff5-a4fd513bcd64] [Fri Nov  3 04:27:47 2023] [x3001c0s13b1n0] - Abort(204544399) (rank 2 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x14637c600000, scount=8448, dtype=0x4c000840, rbuf=0x14637c810000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x14637c600000, scount=8448, dtype=0x4c000840, rbuf=0x14637c810000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 18] [job id 74849796-a6c7-4894-8ff5-a4fd513bcd64] [Fri Nov  3 04:27:47 2023] [x3001c0s1b1n0] - Abort(607197583) (rank 18 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x14f05e400000, scount=8448, dtype=0x4c000840, rbuf=0x14f05e610000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x14f05e400000, scount=8448, dtype=0x4c000840, rbuf=0x14f05e610000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 8] [job id 74849796-a6c7-4894-8ff5-a4fd513bcd64] [Fri Nov  3 04:27:47 2023] [x3001c0s19b1n0] - Abort(540088719) (rank 8 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x147562400000, scount=8448, dtype=0x4c000840, rbuf=0x147562610000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x147562400000, scount=8448, dtype=0x4c000840, rbuf=0x147562610000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 19] [job id 74849796-a6c7-4894-8ff5-a4fd513bcd64] [Fri Nov  3 04:27:47 2023] [x3001c0s1b1n0] - Abort(204544399) (rank 19 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x14a7bc400000, scount=8448, dtype=0x4c000840, rbuf=0x14a7bc610000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x14a7bc400000, scount=8448, dtype=0x4c000840, rbuf=0x14a7bc610000, rcount=8448, datatype=dtype=0x4c000840, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
x3001c0s19b1n0.hsn.cm.polaris.alcf.anl.gov: rank 11 exited with code 255
