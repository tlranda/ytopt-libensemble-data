“RANK= 11 LOCAL_RANK= 3 gpu= 0”
“RANK= 9 LOCAL_RANK= 1 gpu= 2”
“RANK= 8 LOCAL_RANK= 0 gpu= 3”
“RANK= 10 LOCAL_RANK= 2 gpu= 1”
“RANK= 0 LOCAL_RANK= 0 gpu= 3”
“RANK= 1 LOCAL_RANK= 1 gpu= 2”
“RANK= 3 LOCAL_RANK= 3 gpu= 0”
“RANK= 2 LOCAL_RANK= 2 gpu= 1”
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ speed3d_r2c cufft double 128 128 128 -a2a -r2c_dir 2 -ingrid 16 1 1 -outgrid 16 1 1 -n5
Shell debugging restarted
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 128 128 128 -a2a -r2c_dir 2 -ingrid 16 1 1 -outgrid 16 1 1 -n5
+ speed3d_r2c cufft double 128 128 128 -a2a -r2c_dir 2 -ingrid 16 1 1 -outgrid 16 1 1 -n5
+ speed3d_r2c cufft double 128 128 128 -a2a -r2c_dir 2 -ingrid 16 1 1 -outgrid 16 1 1 -n5
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ set +x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
“RANK= 12 LOCAL_RANK= 0 gpu= 3”
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
“RANK= 13 LOCAL_RANK= 1 gpu= 2”
+ '[' -n x ']'
+ set +x
“RANK= 15 LOCAL_RANK= 3 gpu= 0”
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
“RANK= 14 LOCAL_RANK= 2 gpu= 1”
Shell debugging restarted
+ unset __lmod_vx
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 128 128 128 -a2a -r2c_dir 2 -ingrid 16 1 1 -outgrid 16 1 1 -n5
+ speed3d_r2c cufft double 128 128 128 -a2a -r2c_dir 2 -ingrid 16 1 1 -outgrid 16 1 1 -n5
+ speed3d_r2c cufft double 128 128 128 -a2a -r2c_dir 2 -ingrid 16 1 1 -outgrid 16 1 1 -n5
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 128 128 128 -a2a -r2c_dir 2 -ingrid 16 1 1 -outgrid 16 1 1 -n5
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ set +x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ unset __lmod_vx
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ unset __lmod_vx
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 128 128 128 -a2a -r2c_dir 2 -ingrid 16 1 1 -outgrid 16 1 1 -n5
+ speed3d_r2c cufft double 128 128 128 -a2a -r2c_dir 2 -ingrid 16 1 1 -outgrid 16 1 1 -n5
+ speed3d_r2c cufft double 128 128 128 -a2a -r2c_dir 2 -ingrid 16 1 1 -outgrid 16 1 1 -n5
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 128 128 128 -a2a -r2c_dir 2 -ingrid 16 1 1 -outgrid 16 1 1 -n5
“RANK= 4 LOCAL_RANK= 0 gpu= 3”
“RANK= 6 LOCAL_RANK= 2 gpu= 1”
“RANK= 7 LOCAL_RANK= 3 gpu= 0”
“RANK= 5 LOCAL_RANK= 1 gpu= 2”
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 128 128 128 -a2a -r2c_dir 2 -ingrid 16 1 1 -outgrid 16 1 1 -n5
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 128 128 128 -a2a -r2c_dir 2 -ingrid 16 1 1 -outgrid 16 1 1 -n5
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft double 128 128 128 -a2a -r2c_dir 2 -ingrid 16 1 1 -outgrid 16 1 1 -n5
+ speed3d_r2c cufft double 128 128 128 -a2a -r2c_dir 2 -ingrid 16 1 1 -outgrid 16 1 1 -n5
MPICH ERROR [Rank 8] [job id 923147ba-8f91-42fc-b27b-ed5da58d85bc] [Fri Nov  3 04:25:00 2023] [x3003c0s31b0n0] - Abort(607197583) (rank 8 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x15425bc00000, scount=4352, dtype=0x4c001041, rbuf=0x15425bd10000, rcount=4352, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x15425bc00000, scount=4352, dtype=0x4c001041, rbuf=0x15425bd10000, rcount=4352, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)
MPICH ERROR [Rank 0] [job id 923147ba-8f91-42fc-b27b-ed5da58d85bc] [Fri Nov  3 04:25:00 2023] [x3003c0s25b0n0] - Abort(741415311) (rank 0 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x14fb04600000, scount=4352, dtype=0x4c001041, rbuf=0x14fb04710000, rcount=4352, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x14fb04600000, scount=4352, dtype=0x4c001041, rbuf=0x14fb04710000, rcount=4352, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 1] [job id 923147ba-8f91-42fc-b27b-ed5da58d85bc] [Fri Nov  3 04:25:00 2023] [x3003c0s25b0n0] - Abort(3217807) (rank 1 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x14a88a600000, scount=4352, dtype=0x4c001041, rbuf=0x14a88a710000, rcount=4352, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x14a88a600000, scount=4352, dtype=0x4c001041, rbuf=0x14a88a710000, rcount=4352, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 2] [job id 923147ba-8f91-42fc-b27b-ed5da58d85bc] [Fri Nov  3 04:25:00 2023] [x3003c0s25b0n0] - Abort(808524175) (rank 2 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x148e12600000, scount=4352, dtype=0x4c001041, rbuf=0x148e12710000, rcount=4352, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x148e12600000, scount=4352, dtype=0x4c001041, rbuf=0x148e12710000, rcount=4352, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 3] [job id 923147ba-8f91-42fc-b27b-ed5da58d85bc] [Fri Nov  3 04:25:00 2023] [x3003c0s25b0n0] - Abort(271653263) (rank 3 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x14bcc4600000, scount=4352, dtype=0x4c001041, rbuf=0x14bcc4710000, rcount=4352, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x14bcc4600000, scount=4352, dtype=0x4c001041, rbuf=0x14bcc4710000, rcount=4352, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 4] [job id 923147ba-8f91-42fc-b27b-ed5da58d85bc] [Fri Nov  3 04:25:00 2023] [x3003c0s25b1n0] - Abort(472979855) (rank 4 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x148b1a400000, scount=4352, dtype=0x4c001041, rbuf=0x148b1a510000, rcount=4352, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x148b1a400000, scount=4352, dtype=0x4c001041, rbuf=0x148b1a510000, rcount=4352, datatype=dtype=0x4c001041, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 5] [job id 923147ba-8f91-42fc-b27b-ed5da58d85bc] [Fri Nov  3 04:25:00 2023] [x3003c0s25b1n0] - Abort(540088719) (rank 5 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x15453c400000, scount=4352, dtype=0x4c001041, rbuf=0x15453c510000, rcount=4352, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x15453c400000, scount=4352, dtype=0x4c001041, rbuf=0x15453c510000, rcount=4352, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 6] [job id 923147ba-8f91-42fc-b27b-ed5da58d85bc] [Fri Nov  3 04:25:00 2023] [x3003c0s25b1n0] - Abort(674306447) (rank 6 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x1506f4400000, scount=4352, dtype=0x4c001041, rbuf=0x1506f4510000, rcount=4352, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x1506f4400000, scount=4352, dtype=0x4c001041, rbuf=0x1506f4510000, rcount=4352, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 7] [job id 923147ba-8f91-42fc-b27b-ed5da58d85bc] [Fri Nov  3 04:25:00 2023] [x3003c0s25b1n0] - Abort(942741903) (rank 7 in comm 0): Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x1509fc400000, scount=4352, dtype=0x4c001041, rbuf=0x1509fc510000, rcount=4352, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoall: Other MPI error, error stack:
PMPI_Alltoall(433)...............: MPI_Alltoall(sbuf=0x1509fc400000, scount=4352, dtype=0x4c001041, rbuf=0x1509fc510000, rcount=4352, datatype=dtype=0x4c001041, comm=comm=0x84000007) failed
MPIR_CRAY_Alltoall_throttled(471): 
MPIR_Waitall(167)................: 
MPIR_Waitall_impl(51)............: 
MPID_Progress_wait(184)..........: 
MPIDI_Progress_test(80)..........: 
MPIDI_OFI_handle_cq_error(1059)..: OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
x3003c0s31b0n0.hsn.cm.polaris.alcf.anl.gov: rank 8 exited with code 255
