“RANK= 25 LOCAL_RANK= 1 gpu= 2”
“RANK= 27 LOCAL_RANK= 3 gpu= 0”
“RANK= 24 LOCAL_RANK= 0 gpu= 3”
“RANK= 26 LOCAL_RANK= 2 gpu= 1”
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 512 512 512 -no-reorder -r2c_dir 1 -ingrid 8 4 1 -outgrid 4 4 2
+ '[' -z '' ']'
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ speed3d_r2c cufft float 512 512 512 -no-reorder -r2c_dir 1 -ingrid 8 4 1 -outgrid 4 4 2
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 512 512 512 -no-reorder -r2c_dir 1 -ingrid 8 4 1 -outgrid 4 4 2
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 512 512 512 -no-reorder -r2c_dir 1 -ingrid 8 4 1 -outgrid 4 4 2
“RANK= 23 LOCAL_RANK= 3 gpu= 0”
“RANK= 20 LOCAL_RANK= 0 gpu= 3”
“RANK= 21 LOCAL_RANK= 1 gpu= 2”
“RANK= 8 LOCAL_RANK= 0 gpu= 3”
“RANK= 22 LOCAL_RANK= 2 gpu= 1”
“RANK= 11 LOCAL_RANK= 3 gpu= 0”
“RANK= 10 LOCAL_RANK= 2 gpu= 1”
“RANK= 28 LOCAL_RANK= 0 gpu= 3”
“RANK= 30 LOCAL_RANK= 2 gpu= 1”
+ '[' -z '' ']'
+ case "$-" in
“RANK= 31 LOCAL_RANK= 3 gpu= 0”
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
“RANK= 29 LOCAL_RANK= 1 gpu= 2”
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ unset __lmod_vx
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ speed3d_r2c cufft float 512 512 512 -no-reorder -r2c_dir 1 -ingrid 8 4 1 -outgrid 4 4 2
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ unset __lmod_vx
“RANK= 9 LOCAL_RANK= 1 gpu= 2”
+ speed3d_r2c cufft float 512 512 512 -no-reorder -r2c_dir 1 -ingrid 8 4 1 -outgrid 4 4 2
Shell debugging restarted
+ unset __lmod_vx
+ '[' -z '' ']'
+ case "$-" in
Shell debugging restarted
+ unset __lmod_vx
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 512 512 512 -no-reorder -r2c_dir 1 -ingrid 8 4 1 -outgrid 4 4 2
+ speed3d_r2c cufft float 512 512 512 -no-reorder -r2c_dir 1 -ingrid 8 4 1 -outgrid 4 4 2
+ speed3d_r2c cufft float 512 512 512 -no-reorder -r2c_dir 1 -ingrid 8 4 1 -outgrid 4 4 2
+ '[' -z '' ']'
+ case "$-" in
+ speed3d_r2c cufft float 512 512 512 -no-reorder -r2c_dir 1 -ingrid 8 4 1 -outgrid 4 4 2
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ unset __lmod_vx
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ speed3d_r2c cufft float 512 512 512 -no-reorder -r2c_dir 1 -ingrid 8 4 1 -outgrid 4 4 2
Shell debugging restarted
+ unset __lmod_vx
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ unset __lmod_vx
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ speed3d_r2c cufft float 512 512 512 -no-reorder -r2c_dir 1 -ingrid 8 4 1 -outgrid 4 4 2
“RANK= 16 LOCAL_RANK= 0 gpu= 3”
+ '[' -n x ']'
+ set +x
“RANK= 17 LOCAL_RANK= 1 gpu= 2”
Shell debugging restarted
+ unset __lmod_vx
“RANK= 18 LOCAL_RANK= 2 gpu= 1”
+ speed3d_r2c cufft float 512 512 512 -no-reorder -r2c_dir 1 -ingrid 8 4 1 -outgrid 4 4 2
“RANK= 19 LOCAL_RANK= 3 gpu= 0”
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 512 512 512 -no-reorder -r2c_dir 1 -ingrid 8 4 1 -outgrid 4 4 2
+ speed3d_r2c cufft float 512 512 512 -no-reorder -r2c_dir 1 -ingrid 8 4 1 -outgrid 4 4 2
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 512 512 512 -no-reorder -r2c_dir 1 -ingrid 8 4 1 -outgrid 4 4 2
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ speed3d_r2c cufft float 512 512 512 -no-reorder -r2c_dir 1 -ingrid 8 4 1 -outgrid 4 4 2
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 512 512 512 -no-reorder -r2c_dir 1 -ingrid 8 4 1 -outgrid 4 4 2
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 512 512 512 -no-reorder -r2c_dir 1 -ingrid 8 4 1 -outgrid 4 4 2
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 512 512 512 -no-reorder -r2c_dir 1 -ingrid 8 4 1 -outgrid 4 4 2
“RANK= 15 LOCAL_RANK= 3 gpu= 0”
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 512 512 512 -no-reorder -r2c_dir 1 -ingrid 8 4 1 -outgrid 4 4 2
“RANK= 14 LOCAL_RANK= 2 gpu= 1”
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 512 512 512 -no-reorder -r2c_dir 1 -ingrid 8 4 1 -outgrid 4 4 2
“RANK= 12 LOCAL_RANK= 0 gpu= 3”
“RANK= 13 LOCAL_RANK= 1 gpu= 2”
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 512 512 512 -no-reorder -r2c_dir 1 -ingrid 8 4 1 -outgrid 4 4 2
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 512 512 512 -no-reorder -r2c_dir 1 -ingrid 8 4 1 -outgrid 4 4 2
“RANK= 2 LOCAL_RANK= 2 gpu= 1”
“RANK= 3 LOCAL_RANK= 3 gpu= 0”
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -n x ']'
+ set +x
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ speed3d_r2c cufft float 512 512 512 -no-reorder -r2c_dir 1 -ingrid 8 4 1 -outgrid 4 4 2
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 512 512 512 -no-reorder -r2c_dir 1 -ingrid 8 4 1 -outgrid 4 4 2
“RANK= 0 LOCAL_RANK= 0 gpu= 3”
“RANK= 1 LOCAL_RANK= 1 gpu= 2”
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 512 512 512 -no-reorder -r2c_dir 1 -ingrid 8 4 1 -outgrid 4 4 2
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 512 512 512 -no-reorder -r2c_dir 1 -ingrid 8 4 1 -outgrid 4 4 2
“RANK= 5 LOCAL_RANK= 1 gpu= 2”
“RANK= 6 LOCAL_RANK= 2 gpu= 1”
“RANK= 7 LOCAL_RANK= 3 gpu= 0”
+ '[' -z '' ']'
+ case "$-" in
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 512 512 512 -no-reorder -r2c_dir 1 -ingrid 8 4 1 -outgrid 4 4 2
+ '[' -z '' ']'
+ case "$-" in
+ speed3d_r2c cufft float 512 512 512 -no-reorder -r2c_dir 1 -ingrid 8 4 1 -outgrid 4 4 2
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 512 512 512 -no-reorder -r2c_dir 1 -ingrid 8 4 1 -outgrid 4 4 2
“RANK= 4 LOCAL_RANK= 0 gpu= 3”
+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/share/lmod/lmod/init/bash)
+ '[' -n x ']'
+ set +x
Shell debugging restarted
+ unset __lmod_vx
+ speed3d_r2c cufft float 512 512 512 -no-reorder -r2c_dir 1 -ingrid 8 4 1 -outgrid 4 4 2
MPICH ERROR [Rank 5] [job id 029295b6-a4b6-471a-992c-04b424b4f77b] [Sun Jul  2 01:02:00 2023] [x3104c0s19b0n0] - Abort(338762383) (rank 5 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14e128000000, scnts=0x191ca90, sdispls=0x191cf30, dtype=0x4c000840, rbuf=0x14e129000000, rcnts=0x191cf70, rdispls=0x191cf90, datatype=dtype=0x4c000840, comm=comm=0xc4000002) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14e128000000, scnts=0x191ca90, sdispls=0x191cf30, dtype=0x4c000840, rbuf=0x14e129000000, rcnts=0x191cf70, rdispls=0x191cf90, datatype=dtype=0x4c000840, comm=comm=0xc4000002) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 21] [job id 029295b6-a4b6-471a-992c-04b424b4f77b] [Sun Jul  2 01:02:00 2023] [x3104c0s25b0n0] - Abort(405871247) (rank 21 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14e780000000, scnts=0xed8cc0, sdispls=0xe9b4e0, dtype=0x4c000840, rbuf=0x14e781000000, rcnts=0xe9b520, rdispls=0xe9b540, datatype=dtype=0x4c000840, comm=comm=0xc4000002) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14e780000000, scnts=0xed8cc0, sdispls=0xe9b4e0, dtype=0x4c000840, rbuf=0x14e781000000, rcnts=0xe9b520, rdispls=0xe9b540, datatype=dtype=0x4c000840, comm=comm=0xc4000002) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
x3104c0s25b0n0.hsn.cm.polaris.alcf.anl.gov: rank 21 exited with code 255
MPICH ERROR [Rank 31] [job id 029295b6-a4b6-471a-992c-04b424b4f77b] [Sun Jul  2 01:02:00 2023] [x3104c0s31b0n0] - Abort(3218063) (rank 31 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x154e22000000, scnts=0x9cde00, sdispls=0x9904e0, dtype=0x4c000840, rbuf=0x154e23000000, rcnts=0x990520, rdispls=0x990540, datatype=dtype=0x4c000840, comm=comm=0xc4000004) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x154e22000000, scnts=0x9cde00, sdispls=0x9904e0, dtype=0x4c000840, rbuf=0x154e23000000, rcnts=0x990520, rdispls=0x990540, datatype=dtype=0x4c000840, comm=comm=0xc4000004) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 15] [job id 029295b6-a4b6-471a-992c-04b424b4f77b] [Sun Jul  2 01:02:00 2023] [x3104c0s1b0n0] - Abort(808524431) (rank 15 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x148aee000000, scnts=0x1373600, sdispls=0x1373910, dtype=0x4c000840, rbuf=0x148aef000000, rcnts=0x1373950, rdispls=0x1373970, datatype=dtype=0x4c000840, comm=comm=0xc4000004) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x148aee000000, scnts=0x1373600, sdispls=0x1373910, dtype=0x4c000840, rbuf=0x148aef000000, rcnts=0x1373950, rdispls=0x1373970, datatype=dtype=0x4c000840, comm=comm=0xc4000004) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 16] [job id 029295b6-a4b6-471a-992c-04b424b4f77b] [Sun Jul  2 01:02:00 2023] [x3104c0s1b1n0] - Abort(472980111) (rank 16 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14dda6000000, scnts=0x15f4b60, sdispls=0x15f4e70, dtype=0x4c000840, rbuf=0x14dda7040000, rcnts=0x15f5040, rdispls=0x15f5060, datatype=dtype=0x4c000840, comm=comm=0xc4000009) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14dda6000000, scnts=0x15f4b60, sdispls=0x15f4e70, dtype=0x4c000840, rbuf=0x14dda7040000, rcnts=0x15f5040, rdispls=0x15f5060, datatype=dtype=0x4c000840, comm=comm=0xc4000009) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 17] [job id 029295b6-a4b6-471a-992c-04b424b4f77b] [Sun Jul  2 01:02:00 2023] [x3104c0s1b1n0] - Abort(472980111) (rank 17 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x154e02000000, scnts=0xd41a70, sdispls=0xd41f10, dtype=0x4c000840, rbuf=0x154e03040000, rcnts=0xd41f50, rdispls=0xd41f70, datatype=dtype=0x4c000840, comm=comm=0xc4000004) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x154e02000000, scnts=0xd41a70, sdispls=0xd41f10, dtype=0x4c000840, rbuf=0x154e03040000, rcnts=0xd41f50, rdispls=0xd41f70, datatype=dtype=0x4c000840, comm=comm=0xc4000004) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 10] [job id 029295b6-a4b6-471a-992c-04b424b4f77b] [Sun Jul  2 01:02:00 2023] [x3104c0s19b1n0] - Abort(540088975) (rank 10 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x15276e000000, scnts=0x1bd5f90, sdispls=0x1b969d0, dtype=0x4c000840, rbuf=0x15276f000000, rcnts=0x1b96a90, rdispls=0x1b96af0, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x15276e000000, scnts=0x1bd5f90, sdispls=0x1b969d0, dtype=0x4c000840, rbuf=0x15276f000000, rcnts=0x1b96a90, rdispls=0x1b96af0, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 28] [job id 029295b6-a4b6-471a-992c-04b424b4f77b] [Sun Jul  2 01:02:00 2023] [x3104c0s31b0n0] - Abort(137435791) (rank 28 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14d410000000, scnts=0x208add0, sdispls=0x208ae30, dtype=0x4c000840, rbuf=0x14d411000000, rcnts=0x208aef0, rdispls=0x208af50, datatype=dtype=0x4c000840, comm=comm=0xc400000b) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14d410000000, scnts=0x208add0, sdispls=0x208ae30, dtype=0x4c000840, rbuf=0x14d411000000, rcnts=0x208aef0, rdispls=0x208af50, datatype=dtype=0x4c000840, comm=comm=0xc400000b) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 30] [job id 029295b6-a4b6-471a-992c-04b424b4f77b] [Sun Jul  2 01:02:00 2023] [x3104c0s31b0n0] - Abort(741415567) (rank 30 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x151452000000, scnts=0x15dff90, sdispls=0x15a09d0, dtype=0x4c000840, rbuf=0x151453000000, rcnts=0x15a0a90, rdispls=0x15a0af0, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x151452000000, scnts=0x15dff90, sdispls=0x15a09d0, dtype=0x4c000840, rbuf=0x151453000000, rcnts=0x15a0a90, rdispls=0x15a0af0, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 23] [job id 029295b6-a4b6-471a-992c-04b424b4f77b] [Sun Jul  2 01:02:00 2023] [x3104c0s25b0n0] - Abort(338762383) (rank 23 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14ae08000000, scnts=0x241efa0, sdispls=0x2371670, dtype=0x4c000840, rbuf=0x14ae09000000, rcnts=0x23716b0, rdispls=0x23716d0, datatype=dtype=0x4c000840, comm=comm=0xc4000004) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14ae08000000, scnts=0x241efa0, sdispls=0x2371670, dtype=0x4c000840, rbuf=0x14ae09000000, rcnts=0x23716b0, rdispls=0x23716d0, datatype=dtype=0x4c000840, comm=comm=0xc4000004) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 3] [job id 029295b6-a4b6-471a-992c-04b424b4f77b] [Sun Jul  2 01:02:00 2023] [x3104c0s13b1n0] - Abort(942742159) (rank 3 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14f16c000000, scnts=0x139bf90, sdispls=0x13989d0, dtype=0x4c000840, rbuf=0x14f16d040000, rcnts=0x1398a90, rdispls=0x1398af0, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14f16c000000, scnts=0x139bf90, sdispls=0x13989d0, dtype=0x4c000840, rbuf=0x14f16d040000, rcnts=0x1398a90, rdispls=0x1398af0, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 25] [job id 029295b6-a4b6-471a-992c-04b424b4f77b] [Sun Jul  2 01:02:00 2023] [x3104c0s25b1n0] - Abort(405871247) (rank 25 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x149fe8000000, scnts=0x24e93e0, sdispls=0x24e9c20, dtype=0x4c000840, rbuf=0x149fe9000000, rcnts=0x24e9ce0, rdispls=0x24e9d40, datatype=dtype=0x4c000840, comm=comm=0xc4000005) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x149fe8000000, scnts=0x24e93e0, sdispls=0x24e9c20, dtype=0x4c000840, rbuf=0x149fe9000000, rcnts=0x24e9ce0, rdispls=0x24e9d40, datatype=dtype=0x4c000840, comm=comm=0xc4000005) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)
MPICH ERROR [Rank 20] [job id 029295b6-a4b6-471a-992c-04b424b4f77b] [Sun Jul  2 01:02:00 2023] [x3104c0s25b0n0] - Abort(607197839) (rank 20 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14afb8000000, scnts=0xdfdf90, sdispls=0xdfbb90, dtype=0x4c000840, rbuf=0x14afb9000000, rcnts=0xdfbc50, rdispls=0xdfbcb0, datatype=dtype=0x4c000840, comm=comm=0xc400000b) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14afb8000000, scnts=0xdfdf90, sdispls=0xdfbb90, dtype=0x4c000840, rbuf=0x14afb9000000, rcnts=0xdfbc50, rdispls=0xdfbcb0, datatype=dtype=0x4c000840, comm=comm=0xc400000b) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 2] [job id 029295b6-a4b6-471a-992c-04b424b4f77b] [Sun Jul  2 01:02:00 2023] [x3104c0s13b1n0] - Abort(741415567) (rank 2 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x150df6000000, scnts=0x19f5f40, sdispls=0x192ac00, dtype=0x4c000840, rbuf=0x150df7040000, rcnts=0x192acc0, rdispls=0x192ad20, datatype=dtype=0x4c000840, comm=comm=0xc4000004) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x150df6000000, scnts=0x19f5f40, sdispls=0x192ac00, dtype=0x4c000840, rbuf=0x150df7040000, rcnts=0x192acc0, rdispls=0x192ad20, datatype=dtype=0x4c000840, comm=comm=0xc4000004) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 22] [job id 029295b6-a4b6-471a-992c-04b424b4f77b] [Sun Jul  2 01:02:00 2023] [x3104c0s25b0n0] - Abort(942742159) (rank 22 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x145582000000, scnts=0xcb4f30, sdispls=0xc93980, dtype=0x4c000840, rbuf=0x145583000000, rcnts=0xc93a40, rdispls=0xc93aa0, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x145582000000, scnts=0xcb4f30, sdispls=0xc93980, dtype=0x4c000840, rbuf=0x145583000000, rcnts=0xc93a40, rdispls=0xc93aa0, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)
MPICH ERROR [Rank 13] [job id 029295b6-a4b6-471a-992c-04b424b4f77b] [Sun Jul  2 01:02:00 2023] [x3104c0s1b0n0] - Abort(1009851023) (rank 13 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14c39e000000, scnts=0xa52b80, sdispls=0xa34f70, dtype=0x4c000840, rbuf=0x14c39f000000, rcnts=0xa34fb0, rdispls=0xa34fd0, datatype=dtype=0x4c000840, comm=comm=0xc4000002) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14c39e000000, scnts=0xa52b80, sdispls=0xa34f70, dtype=0x4c000840, rbuf=0x14c39f000000, rcnts=0xa34fb0, rdispls=0xa34fd0, datatype=dtype=0x4c000840, comm=comm=0xc4000002) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 7] [job id 029295b6-a4b6-471a-992c-04b424b4f77b] [Sun Jul  2 01:02:00 2023] [x3104c0s19b0n0] - Abort(70326927) (rank 7 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x1550d0000000, scnts=0x1e64be0, sdispls=0x1e274e0, dtype=0x4c000840, rbuf=0x1550d1000000, rcnts=0x1e27520, rdispls=0x1e27540, datatype=dtype=0x4c000840, comm=comm=0xc4000004) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x1550d0000000, scnts=0x1e64be0, sdispls=0x1e274e0, dtype=0x4c000840, rbuf=0x1550d1000000, rcnts=0x1e27520, rdispls=0x1e27540, datatype=dtype=0x4c000840, comm=comm=0xc4000004) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 26] [job id 029295b6-a4b6-471a-992c-04b424b4f77b] [Sun Jul  2 01:02:00 2023] [x3104c0s25b1n0] - Abort(70326927) (rank 26 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14e708000000, scnts=0x833d50, sdispls=0x7f6860, dtype=0x4c000840, rbuf=0x14e709000000, rcnts=0x7f6920, rdispls=0x7f6980, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x14e708000000, scnts=0x833d50, sdispls=0x7f6860, dtype=0x4c000840, rbuf=0x14e709000000, rcnts=0x7f6920, rdispls=0x7f6980, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 6] [job id 029295b6-a4b6-471a-992c-04b424b4f77b] [Sun Jul  2 01:02:00 2023] [x3104c0s19b0n0] - Abort(942742159) (rank 6 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x151602000000, scnts=0x17a2f90, sdispls=0x1763460, dtype=0x4c000840, rbuf=0x151603000000, rcnts=0x1763520, rdispls=0x1763580, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x151602000000, scnts=0x17a2f90, sdispls=0x1763460, dtype=0x4c000840, rbuf=0x151603000000, rcnts=0x1763520, rdispls=0x1763580, datatype=dtype=0x4c000840, comm=comm=0xc4000003) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
MPICH ERROR [Rank 24] [job id 029295b6-a4b6-471a-992c-04b424b4f77b] [Sun Jul  2 01:02:00 2023] [x3104c0s25b1n0] - Abort(607197839) (rank 24 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x151168000000, scnts=0xf10510, sdispls=0xf06b90, dtype=0x4c000840, rbuf=0x151169000000, rcnts=0xf06c50, rdispls=0xf06cb0, datatype=dtype=0x4c000840, comm=comm=0xc400000b) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x151168000000, scnts=0xf10510, sdispls=0xf06b90, dtype=0x4c000840, rbuf=0x151169000000, rcnts=0xf06c50, rdispls=0xf06cb0, datatype=dtype=0x4c000840, comm=comm=0xc400000b) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - local protection error)
MPICH ERROR [Rank 18] [job id 029295b6-a4b6-471a-992c-04b424b4f77b] [Sun Jul  2 01:02:00 2023] [x3104c0s1b1n0] - Abort(1009851023) (rank 18 in comm 0): Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x149fb6000000, scnts=0x1b3d8d0, sdispls=0x1b00710, dtype=0x4c000840, rbuf=0x149fb7040000, rcnts=0x1b007d0, rdispls=0x1b00830, datatype=dtype=0x4c000840, comm=comm=0xc4000004) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)

aborting job:
Fatal error in PMPI_Alltoallv: Other MPI error, error stack:
PMPI_Alltoallv(389)............: MPI_Alltoallv(sbuf=0x149fb6000000, scnts=0x1b3d8d0, sdispls=0x1b00710, dtype=0x4c000840, rbuf=0x149fb7040000, rcnts=0x1b007d0, rdispls=0x1b00830, datatype=dtype=0x4c000840, comm=comm=0xc4000004) failed
MPIR_CRAY_Alltoallv(1162)......: 
MPIR_Waitall(167)..............: 
MPIR_Waitall_impl(51)..........: 
MPID_Progress_wait(184)........: 
MPIDI_Progress_test(80)........: 
MPIDI_OFI_handle_cq_error(1059): OFI poll failed (ofi_events.c:1061:MPIDI_OFI_handle_cq_error:Input/output error - remote access error)
